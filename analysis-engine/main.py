from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import numpy as np
import json
import re
import ast
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager
import os
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
import psycopg2

# =========================================================
# Insight Filtering Constants
# =========================================================

# ê¸°ì¡´ ë²”ìš© ë¶ˆìš©ì–´ (êµ¬ë§¤/ì‚¬ìš© í–‰ìœ„ ê´€ë ¨)
GENERIC_INSIGHT_STOPWORDS = {
    'good', 'great', 'nice', 'excellent', 'better', 'perfect', 'best', 'like', 'love',
    'really', 'much', 'get', 'well', 'buy', 'purchased', 'recommend', 'recommended',
    'happy', 'satisfied', 'product', 'item', 'bought', 'try', 'tried', 'use', 'used',
    'using', 'add', 'added', 'adding', 'recipe', 'highly', 'food', 'definitely',
    'amazing', 'awesome', 'wonderful', 'bit', 'lot', 'little',
    'ordered', 'received', 'came', 'made', 'make', 'makes', 'everything', 'everyone',
    'anyone', 'anything', 'would', 'could', 'should', 'first', 'second', 'ever', 'never'
}

# [ì‹ ê·œ] ê°ê°/ê°ì • ê´€ë ¨ ë¶ˆìš©ì–´ â€” "ë»”í•œ ì¹­ì°¬" ì„ ë¶„ì„ ëŒ€ìƒì—ì„œ ì œê±°
SENSORY_STOPWORDS = {
    'taste', 'tastes', 'tasted', 'flavor', 'flavour', 'smell', 'smells', 'scent',
    'delicious', 'yummy', 'tasty', 'good', 'great', 'bad', 'horrible', 'best',
    'amazing', 'awesome', 'love', 'like', 'really', 'much', 'perfect', 'nice',
    'better', 'excellent', 'favorite', 'quality', 'product', 'item', 'buy', 'buying',
    'bought', 'order', 'ordered', 'definitely', 'highly', 'recommend', 'worth'
}

# í†µí•© ë¶ˆìš©ì–´ (CountVectorizer ì „ë‹¬ìš©) â€” sklearn ê¸°ë³¸ + ì»¤ìŠ¤í…€
COMBINED_STOP_WORDS = list(ENGLISH_STOP_WORDS | GENERIC_INSIGHT_STOPWORDS | SENSORY_STOPWORDS)

# ìˆœìˆ˜ ì‹ê° í˜•ìš©ì‚¬ (ê°€ì¤‘ì¹˜ ëŒ€ìƒ â€” í‰ê°€ì„± ë‹¨ì–´ ì œê±°)
SENSORY_KEYWORDS = {
    'spicy', 'hot', 'sweet', 'savory', 'crunchy', 'crispy', 'salty', 'bitter', 'sour',
    'tangy', 'garlicky', 'smoky', 'smooth', 'creamy', 'chewy', 'tender', 'fresh', 'mild',
    'strong', 'rich', 'bold', 'dark', 'light', 'kick', 'burn', 'acid'
}

# ì‹ê° ì „ìš© í‚¤ì›Œë“œ ì‚¬ì „ (extract_specific_insightsì—ì„œ ì‚¬ìš©)
TEXTURE_KEYWORDS = {
    'crunchy', 'crispy', 'chewy', 'soft', 'spicy', 'salty', 'sweet', 'sour',
    'thick', 'thin', 'rich', 'creamy', 'juicy', 'dry', 'moist', 'greasy'
}

# í˜ì–´ë§ ì¬ë£Œ í‚¤ì›Œë“œ
PAIRING_KEYWORDS = {
    'rice', 'noodle', 'noodles', 'chicken', 'meat', 'beef', 'pork', 'pizza', 'sandwich',
    'salad', 'soup', 'topping', 'toppings', 'dip', 'sauce', 'stew', 'fried', 'grilled',
    'bread', 'vegetable', 'vegetables', 'eggs', 'steak', 'burger', 'taco', 'tacos'
}

# í˜ì–´ë§ ë¬¸ë§¥ ë§ˆì»¤ (ì „ì¹˜ì‚¬/ë™ì‚¬)
PAIRING_MARKERS = {'with', 'add', 'on', 'mix', 'top', 'serve'}


def is_generic_term(term):
    """í‚¤ì›Œë“œê°€ ë‹¨ì¼ ë²”ìš© ë‹¨ì–´ì´ê±°ë‚˜, Bigramì˜ ëª¨ë“  ë‹¨ì–´ê°€ ë²”ìš©+ê°ê° ë¶ˆìš©ì–´ì¸ ê²½ìš° True ë°˜í™˜"""
    words = term.lower().split()
    all_stopwords = GENERIC_INSIGHT_STOPWORDS | SENSORY_STOPWORDS
    return all(w in all_stopwords for w in words)


def calculate_relevance_score(keyword, mention_count, impact_score):
    """í‚¤ì›Œë“œì˜ ì˜ë¯¸ì  ê°€ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚° (Impact Score ë°˜ì˜ ê³ ë„í™”)"""
    words = keyword.lower().split()

    frequency_score = np.log1p(float(mention_count))
    impact_weight = 1.0 + (abs(impact_score) * 1.5)

    has_sensory = any(w in SENSORY_KEYWORDS for w in words)  # ìˆœìˆ˜ ì‹ê°ë§Œ
    has_pairing = any(w in PAIRING_KEYWORDS for w in words)

    multiplier = 1.0
    if has_sensory: multiplier *= 1.8
    if has_pairing: multiplier *= 1.4

    # ë²”ìš© ë‹¨ì–´ í˜ë„í‹° (ê°ê°/ì¬ë£Œ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°)
    all_stop = GENERIC_INSIGHT_STOPWORDS | SENSORY_STOPWORDS
    has_generic = any(w in all_stop for w in words)
    if has_generic and not (has_sensory or has_pairing):
        multiplier *= 0.3  # ë” ê°•í•œ í˜ë„í‹°

    return frequency_score * impact_weight * multiplier


def analyze_features(df_filtered):
    """DB ì»¬ëŸ¼(texture_terms, ingredients)ì„ í™œìš©í•œ ì‹ê°/í˜ì–´ë§ ë¶„ì„
    
    CountVectorizer ëŒ€ì‹  ì´ë¯¸ ì •ì œëœ ì»¬ëŸ¼ ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ì—¬ ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µ
    """
    result = {"top_textures": [], "top_pairings": []}

    # 1. ì‹ê° (Texture) ë¶„ì„ â€” texture_terms ì»¬ëŸ¼ í™œìš©
    if 'texture_terms' in df_filtered.columns:
        all_textures = []
        for terms in df_filtered['texture_terms'].dropna():
            try:
                parsed = terms if isinstance(terms, list) else ast.literal_eval(str(terms))
                # _ADJ ë“± íƒœê·¸ ì œê±°, ë¹ˆ ë¬¸ìì—´ ìŠ¤í‚µ
                cleaned = [t.split('_')[0].lower() for t in parsed if t and isinstance(t, str)]
                all_textures.extend(cleaned)
            except (ValueError, SyntaxError):
                pass
        if all_textures:
            result["top_textures"] = [{'term': t, 'count': c} for t, c in Counter(all_textures).most_common(8)]

    # 2. ì¬ë£Œ/í˜ì–´ë§ (Ingredients) ë¶„ì„ â€” ingredients ì»¬ëŸ¼ í™œìš©
    if 'ingredients' in df_filtered.columns:
        all_ingredients = []
        for ing_list in df_filtered['ingredients'].dropna():
            try:
                parsed = ing_list if isinstance(ing_list, list) else ast.literal_eval(str(ing_list))
                for item in parsed:
                    if isinstance(item, str) and len(item) > 2:
                        clean_item = item.split('_')[0].lower()
                        # NOT_ ì ‘ë‘ì–´ ì œê±°, ë¶ˆìš©ì–´ ì œì™¸
                        if clean_item.startswith('not_'):
                            continue
                        if clean_item not in SENSORY_STOPWORDS and clean_item not in GENERIC_INSIGHT_STOPWORDS:
                            all_ingredients.append(clean_item)
            except (ValueError, SyntaxError):
                pass
        if all_ingredients:
            result["top_pairings"] = [{'term': t, 'count': c} for t, c in Counter(all_ingredients).most_common(8)]

    return result


def extract_specific_insights(texts, mode='pairing'):
    """í…ìŠ¤íŠ¸ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì‹ê°/í˜ì–´ë§ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ (DB ì»¬ëŸ¼ ë³´ì™„ìš©)
    
    mode='pairing': 'with', 'add', 'mix' ë’¤ì— ë‚˜ì˜¤ëŠ” ëª…ì‚¬(ì¬ë£Œ) ì¶”ì¶œ
    mode='texture': ì‹ê° í˜•ìš©ì‚¬ê°€ í¬í•¨ëœ ë¬¸êµ¬ ì¶”ì¶œ
    """
    extracted = []

    for text in texts:
        text = str(text).lower()
        words = text.split()

        if mode == 'pairing':
            for i, word in enumerate(words[:-1]):
                if word in PAIRING_MARKERS:
                    target = words[i + 1]
                    if len(target) > 2 and target not in SENSORY_STOPWORDS:
                        extracted.append(f"{word} {target}")

        elif mode == 'texture':
            for i, word in enumerate(words):
                if word in TEXTURE_KEYWORDS:
                    prev = words[i - 1] if i > 0 else ""
                    phrase = f"{prev} {word}".strip()
                    extracted.append(phrase)

    return [{'term': t, 'count': c} for t, c in Counter(extracted).most_common(5)]

# DB ì—°ê²° 
def parse_spring_datasource_url(url):
    """Parse jdbc:postgresql://host:port/database?params format"""
    if not url:
        return None, None, None
    # Pattern: jdbc:postgresql://host:port/database
    match = re.match(r'jdbc:postgresql://([^:]+):(\d+)/([^?]+)', url)
    if match:
        return match.group(1), match.group(2), match.group(3)
    return None, None, None

# Try Spring format first, fall back to legacy format
SPRING_URL = os.environ.get("SPRING_DATASOURCE_URL", "")
_parsed_host, _parsed_port, _parsed_db = parse_spring_datasource_url(SPRING_URL)

DB_HOST = _parsed_host or os.environ.get("DB_HOST", "db")
DB_PORT = _parsed_port or os.environ.get("DB_PORT", "5432")
DB_NAME = _parsed_db or os.environ.get("POSTGRES_DB", "bigproject")
DB_USER = os.environ.get("SPRING_DATASOURCE_USERNAME") or os.environ.get("POSTGRES_USER", "postgres")
DB_PASS = os.environ.get("SPRING_DATASOURCE_PASSWORD") or os.environ.get("POSTGRES_PASSWORD", "postgres")

def get_db_connection():
    try:
        conn = psycopg2.connect(
            host=DB_HOST,
            dbname=DB_NAME,
            user=DB_USER,
            password=DB_PASS,
            port=DB_PORT,
            sslmode=os.environ.get("DB_SSLMODE", "require")
        )
        return conn
    except Exception as e:
        print(f"DB Connection Failed: {e}")
        return None

# êµ­ê°€ ë§¤í•‘
COUNTRY_MAPPING = {
    'ë¯¸êµ­': 'US',
    'ì¤‘êµ­': 'CN',
    'ì¼ë³¸': 'JP',
    'ë² íŠ¸ë‚¨': 'VN',
    'ë…ì¼': 'DE'
}

REVERSE_MAPPING = {v: k for k, v in COUNTRY_MAPPING.items()} # {'US': 'ë¯¸êµ­', ...}

# UI í‘œì‹œ ì´ë¦„ -> CSV ì €ì¥ ì´ë¦„ ë§¤í•‘
UI_TO_CSV_ITEM_MAPPING = {
    "ê°„ì¥": "ê°„ì¥", "ê°": "ê°", "ê±´ê³ ì‚¬ë¦¬": "ê³ ì‚¬ë¦¬", "ê³ ì¶”ì¥": "ê³ ì¶”ì¥", "êµ­ìˆ˜": "êµ­ìˆ˜",
    "ì°¸ì¹˜ í†µì¡°ë¦¼": "ê¸°ë¦„ì— ë‹´ê·¼ ê²ƒ", "ê¹€ì¹˜": "ê¹€ì¹˜", "ê¹ë§ˆëŠ˜": "ê»ì§ˆì„ ê¹ ê²ƒ", "ê¹€ë°¥ë¥˜": "ëƒ‰ë™ê¹€ë°¥",
    "ëƒ‰ë©´": "ëƒ‰ë©´", "ë‹¹ë©´": "ë‹¹ë©´", "ê±´ë”ë•": "ë”ë•", "ëœì¥": "ëœì¥", "ë‘ë¶€": "ë‘ë¶€",
    "ë“¤ê¸°ë¦„": "ë“¤ê¸°ë¦„ê³¼ ê·¸ ë¶„íšë¬¼", "ë¼ë©´": "ë¼ë©´", "ìŒ€": "ë©¥ìŒ€", "í”¼í´ ë° ì ˆì„ì±„ì†Œ": "ë°€íìš©ê¸°ì— ë„£ì€ ê²ƒ",
    "ëƒ‰ë™ ë°¤": "ë°¤", "ì¿ í‚¤ ë° í¬ë˜ì»¤": "ë¹„ìŠ¤í‚·, ì¿ í‚¤ì™€ í¬ë˜ì»¤", "ì‚¼ê³„íƒ•": "ì‚¼ê³„íƒ•", "ì†Œì‹œì§€": "ì†Œì‹œì§€",
    "ì†Œì£¼": "ì†Œì£¼", "ë§Œë‘": "ì†ì„ ì±„ìš´ íŒŒìŠ¤íƒ€(ì¡°ë¦¬í•œ ê²ƒì¸ì§€ ë˜ëŠ” ê·¸ ë°–ì˜ ë°©ë²•ìœ¼ë¡œ ì¡°ì œí•œ ê²ƒì¸ì§€ì— ìƒê´€ì—†ë‹¤)",
    "ì´ˆì½”íŒŒì´ë¥˜": "ìŠ¤ìœ„íŠ¸ ë¹„ìŠ¤í‚·", "ë–¡ë³¶ì´ ë–¡": "ìŒ€ê°€ë£¨ì˜ ê²ƒ", "ì „í†µ í•œê³¼/ì•½ê³¼": "ìŒ€ê³¼ì", "ìœ ì": "ìœ ì",
    "ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼": "ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼ì˜ ì¡°ì œí’ˆ", "ì¦‰ì„ë°¥": "ì°Œê±°ë‚˜ ì‚¶ì€ ìŒ€", "ì°¸ê¸°ë¦„": "ì°¸ê¸°ë¦„ê³¼ ê·¸ ë¶„íšë¬¼",
    "ë§‰ê±¸ë¦¬": "íƒì£¼", "ìŒ€ íŠ€ë°¥": "íŠ€ê¸´ ìŒ€", "íŒ½ì´ë²„ì„¯": "íŒ½ì´ë²„ì„¯", 
    "í‘œê³ ë²„ì„¯": "í‘œê³ ë²„ì„¯", "ìŒˆì¥ ë° ì–‘ë…ì¥": "í˜¼í•©ì¡°ë¯¸ë£Œ", "í™ì‚¼ ì—‘ê¸°ìŠ¤": "í™ì‚¼ ì¶”ì¶œë¬¼(extract)"
}

CSV_TO_UI_ITEM_MAPPING = {v: k for k, v in UI_TO_CSV_ITEM_MAPPING.items()}

# ì•„ì´í…œë³„ ê²€ìƒ‰ì–´(Trend Keyword) ë§¤í•‘
# íŠ¸ë Œë“œ ë°ì´í„° ì»¬ëŸ¼ëª… ì˜ˆì‹œ: {COUNTRY}_{KEYWORD}_mean
ITEM_TO_TREND_MAPPING = {
    # [Level 3] í•µì‹¬ ì†ŒìŠ¤ ë° ì¥ë¥˜
    "ê°„ì¥": "SoySauce",          # (ìˆ˜ì •) Gochujang -> SoySauce (ì§ì ‘ ë§¤í•‘ ê°€ëŠ¥)
    "ê³ ì¶”ì¥": "Gochujang",
    "ëœì¥": "Doenjang",
    "ìŒˆì¥ ë° ì–‘ë…ì¥": "Ssamjang",
    "ê¹€ì¹˜": "Kimchi",
    "ì°¸ê¸°ë¦„": "SesameOil",       # (ì¶”ê°€) ìƒˆ íŠ¸ë Œë“œ í‚¤ SesameOil ë°˜ì˜
    
    "ë¼ë©´": "Ramen",             # (ìˆ˜ì •) Ramyun -> Ramen (íŠ¸ë Œë“œ í‚¤ ì´ë¦„ ì¼ì¹˜)
    "êµ­ìˆ˜": "Ramen",             # ë©´ë¥˜ íŠ¸ë Œë“œ ëŒ€í‘¯ê°’ìœ¼ë¡œ Ramen í™œìš©
    "ëƒ‰ë©´": "Ramen",
    "ë‹¹ë©´": "GlassNoodles",      # (ìˆ˜ì •) Ramen -> GlassNoodles (ì •í™•í•œ ë§¤í•‘)
    "ë§Œë‘": "Mandu",             # (ì¶”ê°€) KFood -> Mandu ë°˜ì˜
    "ì¦‰ì„ë°¥": "InstantRice",     # (ì¶”ê°€) KFood -> InstantRice ë°˜ì˜
    "ë–¡ë³¶ì´ ë–¡": "RiceCake",      # (ìˆ˜ì •) Tteokbokki(ìš”ë¦¬) ëŒ€ì‹  RiceCake(ì¬ë£Œ) ë§¤í•‘
    "íŒ½ì´ë²„ì„¯": "Enoki",         # (ì¶”ê°€) KFood -> Enoki ë°˜ì˜
    
    # [Level 2] ë°”ì´ëŸ´ ë©”ë‰´ (Viral Menu)
    "ê¹€ë°¥ë¥˜": "Kimbap",          # (ìˆ˜ì •) Gimbap -> Kimbap (íŠ¸ë Œë“œ í‚¤ ì´ë¦„ ì¼ì¹˜)
    
    # [Level 5] ê³ ë¶€ê°€ê°€ì¹˜ ë° ê±´ê°•ì‹í’ˆ
    "ìœ ì": "Yuja",
    "í™ì‚¼ ì—‘ê¸°ìŠ¤": "Ginseng",    # (ì¶”ê°€) KFood -> Ginseng ë°˜ì˜
    "ë“¤ê¸°ë¦„": "PerillaOil",      # (ì¶”ê°€) KFood -> PerillaOil ë°˜ì˜

    # [Level 1] íŠ¸ë Œë“œ í‚¤ê°€ ì—†ëŠ” ê²½ìš° ìƒìœ„ ë²”ì£¼(KFood)ë¡œ ë§¤í•‘
    "ì†Œì£¼": "KFood",             # (ì°¸ê³ ) Soju íŠ¸ë Œë“œ ë¶„ì„ ì œì™¸ë¨
    "ë§‰ê±¸ë¦¬": "KFood",           # (ì°¸ê³ ) Makgeolli íŠ¸ë Œë“œ ë¶„ì„ ì œì™¸ë¨
    "ì‚¼ê³„íƒ•": "KFood",
    "ì°¸ì¹˜ í†µì¡°ë¦¼": "KFood",
    "ì´ˆì½”íŒŒì´ë¥˜": "KFood",
    "ì¿ í‚¤ ë° í¬ë˜ì»¤": "KFood",
    "ì „í†µ í•œê³¼/ì•½ê³¼": "KFood",
    "ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼": "KFood",
    "ìŒ€": "KFood",
    "ë‘ë¶€": "KFood",
    "í‘œê³ ë²„ì„¯": "KFood"
}

df = None
growth_summary_df = None
df_consumer = None
GLOBAL_MEAN_SENTIMENT = 0.5
GLOBAL_STD_SENTIMENT = 0.3
GLOBAL_MEAN_RATING = 3.0

# =============================================================================
# í—¬í¼ í•¨ìˆ˜: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë¶„ì„ ì§€í‘œ ê³„ì‚°
# =============================================================================

def remove_pos_tags(text: str) -> str:
    """cleaned_textì—ì„œ _NOUN, _ADJ, _VERB ë“± í’ˆì‚¬ íƒœê·¸ ì œê±°
    
    Example: 'taste_NOUN good_ADJ' -> 'taste good'
    """
    if not isinstance(text, str):
        return ""
    return re.sub(r'_[A-Z]+', '', text)


def extract_bigrams_with_metrics(
    texts: pd.Series, 
    ratings: pd.Series, 
    original_texts: pd.Series,
    top_n: int = 15,
    adj_priority: bool = True,
    min_df: int = 5
) -> List[Dict[str, Any]]:
    """
    Bigram ì¶”ì¶œ í›„ Impact Score, Positivity Rate ê³„ì‚°.
    í˜•ìš©ì‚¬(_ADJ) í¬í•¨ ì¡°í•©ì„ ìš°ì„ ìˆœìœ„ë¡œ ì œì•ˆ.
    
    Args:
        texts: cleaned_text ì»¬ëŸ¼ (í’ˆì‚¬ íƒœê·¸ í¬í•¨)
        ratings: rating ì»¬ëŸ¼
        original_texts: original_text ì»¬ëŸ¼ (Drill-downìš©)
        top_n: ë°˜í™˜í•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
        adj_priority: í˜•ìš©ì‚¬ í¬í•¨ Bigramë§Œ ë…¸ì¶œí• ì§€ ì—¬ë¶€ (Falseë©´ ëª¨ë“  Bigram ë…¸ì¶œ)
        min_df: CountVectorizerì˜ ìµœì†Œ ë“±ì¥ ë¹ˆë„
    
    Returns:
        List of keyword analysis dicts with impact_score, positivity_rate, sample_reviews
    """
    if texts.empty:
        return []
    
    # 1. Bigram ì¶”ì¶œ
    try:
        # í’ˆì‚¬ íƒœê·¸ ì œê±°ëœ í…ìŠ¤íŠ¸ë¡œ Bigram ì¶”ì¶œ
        cleaned_texts_no_tags = texts.apply(remove_pos_tags).fillna('')
        
        vectorizer = CountVectorizer(
            ngram_range=(2, 2),
            min_df=min_df,
            max_features=1000, # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ì¶”ì¶œ
            stop_words=COMBINED_STOP_WORDS,
            token_pattern=r'\b[a-zA-Z]{3,}\b'
        )
        
        bigram_matrix = vectorizer.fit_transform(cleaned_texts_no_tags)
        raw_bigram_names = vectorizer.get_feature_names_out()
        raw_bigram_counts = bigram_matrix.sum(axis=0).A1
        
        # ë²”ìš© ë‹¨ì–´ í•„í„°ë§ ì ìš©
        bigram_names = []
        bigram_counts = []
        for i, name in enumerate(raw_bigram_names):
            if not is_generic_term(name):
                bigram_names.append(name)
                bigram_counts.append(raw_bigram_counts[i])
        
    except Exception as e:
        print(f"Bigram ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []
    
    # 2. í˜•ìš©ì‚¬ í¬í•¨ Bigram í•„í„°ë§ (ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ _ADJ íƒœê·¸ í™•ì¸)
    adj_bigrams = set()
    if adj_priority:
        # ì£¼ì„: textsëŠ” íƒœê·¸ê°€ í¬í•¨ëœ cleaned_text ì»¬ëŸ¼ì„
        try:
            all_text = " ".join(texts.dropna().astype(str))
            for bigram in bigram_names:
                words = bigram.split()
                if len(words) == 2:
                    if f"{words[0]}_ADJ" in all_text or f"{words[1]}_ADJ" in all_text:
                        adj_bigrams.add(bigram)
        except Exception as e:
            print(f"í˜•ìš©ì‚¬ í•„í„°ë§ ì˜¤ë¥˜: {e}")

    # 3. ê° Bigramì— ëŒ€í•´ Impact Score, Positivity Rate ê³„ì‚°
    results = []
    
    for idx, bigram in enumerate(bigram_names):
        count = int(bigram_counts[idx])
        if count < min_df: # min_dfë³´ë‹¤ ì ìœ¼ë©´ pass (CountVectorizerì—ì„œ ì´ë¯¸ ê±¸ëŸ¬ì¡Œê² ì§€ë§Œ ì•ˆì „ì¥ì¹˜)
            continue
            
        # í•´ë‹¹ Bigramì„ í¬í•¨í•˜ëŠ” ë¦¬ë·° í•„í„°ë§
        # cleaned_texts_no_tagsë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
        mask = cleaned_texts_no_tags.str.contains(bigram, case=False, na=False, regex=False)
        matching_ratings = ratings[mask]
        matching_originals = original_texts[mask]
        
        if len(matching_ratings) == 0:
            continue
        
        # Impact Score = í•´ë‹¹ í‚¤ì›Œë“œ í¬í•¨ í‰ê·  - ì „ì²´ í‰ê· (3.0)
        avg_rating = matching_ratings.mean()
        impact_score = round(avg_rating - 3.0, 2)
        
        # Positivity Rate = 4-5ì  ë¹„ìœ¨ (%)
        positive_count = (matching_ratings >= 4).sum()
        positivity_rate = round((positive_count / len(matching_ratings)) * 100, 1)
        
        # Satisfaction Index = (5ì  ë¦¬ë·° ë¹„ìœ¨) / 0.2 (ì „ì²´ 5ì  í™•ë¥ )
        # FLOAT ì˜¤ì°¨ ë°©ì§€ë¥¼ ìœ„í•´ 4.9 ì´ìƒìœ¼ë¡œ ì²´í¬
        five_star_ratio = (matching_ratings >= 4.9).mean()
        satisfaction_index = round(five_star_ratio / 0.2, 2)
        
        # Sample Reviews (ìµœëŒ€ 3ê°œ)
        sample_reviews = matching_originals.dropna().head(3).tolist()
        
        # í˜•ìš©ì‚¬ í¬í•¨ ì—¬ë¶€
        has_adj = bigram in adj_bigrams
        
        results.append({
            "keyword": bigram,
            "impact_score": impact_score,
            "positivity_rate": positivity_rate, # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€ (API ì“°ëŠ” ë‹¤ë¥¸ ê³³ì´ ìˆì„ ìˆ˜ ìˆìŒ)
            "satisfaction_index": satisfaction_index,
            "mention_count": count,
            "sample_reviews": sample_reviews,
            "has_adjective": has_adj
        })
    
    # 4. ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì •ë ¬: Relevance Score ê³„ì‚° ë° ì •ë ¬
    for r in results:
        r["relevance_score"] = calculate_relevance_score(r["keyword"], r["mention_count"], r["impact_score"])
    
    # 1ì°¨ ì •ë ¬: Relevance Score (ë†’ì€ ìˆœ), 2ì°¨ ì •ë ¬: ì–¸ê¸‰ íšŸìˆ˜
    results = sorted(results, key=lambda x: (-x["relevance_score"], -x["mention_count"]))
    
    return results[:top_n]


def get_diverging_keywords(keywords_analysis: List[Dict], top_n: int = 10, threshold: float = 0.3) -> Dict[str, List[Dict]]:
    """
    Impact Score ê¸°ì¤€ìœ¼ë¡œ ë¶€ì •/ê¸ì • í‚¤ì›Œë“œ ë¶„ë¦¬
    
    Args:
        keywords_analysis: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        top_n: ê²°ê³¼ë‹¹ ìµœëŒ€ ê°œìˆ˜
        threshold: í•„í„°ë§í•  Impact Scoreì˜ ì ˆëŒ€ê°’ ë¬¸í„± (ë°ì´í„° ì ìœ¼ë©´ 0.0)
    
    Returns:
        {"negative": [...], "positive": [...]}
    """
    # ë¶€ì • í‚¤ì›Œë“œ: impact_score < -threshold
    negative = sorted(
        [k for k in keywords_analysis if k["impact_score"] < -threshold],
        key=lambda x: x["impact_score"]
    )[:top_n]
    
    # ê¸ì • í‚¤ì›Œë“œ: impact_score > threshold
    positive = sorted(
        [k for k in keywords_analysis if k["impact_score"] > threshold],
        key=lambda x: -x["impact_score"]
    )[:top_n]
    
    return {"negative": negative, "positive": positive}

import threading
import time

def load_data_background():
    global df, growth_summary_df
    global GLOBAL_MEAN_SENTIMENT, GLOBAL_STD_SENTIMENT, GLOBAL_MEAN_RATING

    print("ğŸš€ [Background] Starting Data Loading...", flush=True)
    
    # Retry mechanism: Wait for DB migration if needed (Up to 5 minutes)
    max_retries = 60 
    for i in range(max_retries):
        conn = get_db_connection()
        if conn:
            try:
                # 1. Load Export Trends
                print(f"Loading export_trends from DB (Attempt {i+1}/{max_retries})...", flush=True)
                query = "SELECT * FROM export_trends"
                
                # [Fix] Use cursor directly to avoid Pandas/SQLAlchemy warning and potential hang
                with conn.cursor() as cur:
                    cur.execute(query)
                    if cur.description:
                        cols = [desc[0] for desc in cur.description]
                        rows = cur.fetchall()
                        temp_df = pd.DataFrame(rows, columns=cols)
                    else:
                        temp_df = pd.DataFrame()

                print(f"Loaded {len(temp_df)} rows. Processing...", flush=True)
                
                if not temp_df.empty:
                    # [Optimization] Do NOT expand JSONB trend_data globally at startup.
                    df = temp_df

                    # Ensure trend_data is parsed as dict (if it comes as string)
                    if 'trend_data' in df.columns:
                        print("Processing trend_data column...", flush=True)
                        def ensure_dict(x):
                            if isinstance(x, dict): return x
                            if isinstance(x, str):
                                try: return json.loads(x)
                                except: return {}
                            return {}
                        df['trend_data'] = df['trend_data'].apply(ensure_dict)

                    # Numeric Cleanups
                    print("Performing numeric cleanup...", flush=True)
                    
                    # Explicit type conversion for critical columns to avoid object dtype issues
                    numeric_targets = ['export_value', 'export_weight', 'unit_price', 'exchange_rate', 'gdp_level', 'cpi']
                    for col in numeric_targets:
                        if col in df.columns:
                            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

                    numeric_cols = df.select_dtypes(include=[np.number]).columns
                    df[numeric_cols] = df[numeric_cols].fillna(0)
                    
                    # Growth Matrix Calculation
                    print("Calculating Growth Matrix...", flush=True)
                    summaries = []
                    group_cols = ['country_code', 'item_name']
                    if 'country_code' not in df.columns:
                            group_cols = ['country_name', 'item_name']
                            
                    grouped = df.groupby(group_cols)
                    for name, group in grouped:
                        if len(group) < 24: continue
                        group = group.sort_values('period_str')
                        recent_12 = group.tail(12)
                        prev_12 = group.iloc[-24:-12]
                        
                        weight_col = 'export_weight' if 'export_weight' in group.columns else None
                        if weight_col:
                            w_curr = recent_12[weight_col].sum()
                            w_prev = prev_12[weight_col].sum()
                        else: 
                                w_curr = recent_12['export_value'].sum()
                                w_prev = prev_12['export_value'].sum()
        
                        weight_growth = ((w_curr - w_prev) / w_prev * 100) if w_prev > 0 else 0
                        
                        p_curr = recent_12['unit_price'].mean()
                        p_prev = prev_12['unit_price'].mean()
                        price_growth = ((p_curr - p_prev) / p_prev * 100) if p_prev > 0 else 0
                        
                        total_value = recent_12['export_value'].sum()
                        
                        summaries.append({
                            'country': name[0] if 'country_code' in df.columns else COUNTRY_MAPPING.get(name[0], name[0]),
                            'item_csv_name': name[1],
                            'weight_growth': round(weight_growth, 1),
                            'price_growth': round(price_growth, 1),
                            'total_value': total_value
                        })
                    growth_summary_df = pd.DataFrame(summaries)
                    print("Export Trends Loaded & Matrix Calculated.", flush=True)
                    
                    # 2. Global Consumer Stats (Only if step 1 success)
                    print("Calculating Global Consumer Stats from DB...", flush=True)
                    with conn.cursor() as cur:
                        try:
                            cur.execute("SELECT AVG(sentiment_score), STDDEV(sentiment_score), AVG(rating) FROM amazon_reviews")
                            row = cur.fetchone()
                            if row and row[0] is not None:
                                    GLOBAL_MEAN_SENTIMENT = float(row[0])
                                    GLOBAL_STD_SENTIMENT = float(row[1]) if row[1] is not None else 0.3
                                    GLOBAL_MEAN_RATING = float(row[2])
                                    print(f"Global Stats: Sent={GLOBAL_MEAN_SENTIMENT:.2f}, Std={GLOBAL_STD_SENTIMENT:.2f}, Rating={GLOBAL_MEAN_RATING:.2f}", flush=True)
                            else:
                                    print("âš ï¸ amazon_reviews table empty or stats unavailable.", flush=True)
                        except Exception as ex:
                            print(f"Global Stats calculation failed: {ex}", flush=True)
                    
                    conn.close()
                    break # Success, exit retry loop
                    
                else:
                    print(f"âš ï¸ export_trends table is empty. Migration might be in progress... (Attempt {i+1}/{max_retries})", flush=True)
                    conn.close()
                    time.sleep(5) # Wait for migration

            except Exception as e:
                print(f"DB Load Failed (Attempt {i+1}/{max_retries}): {e}", flush=True)
                if conn: conn.close()
                time.sleep(5) # Wait before retry
        else:
            print(f"DB Connection Failed (Attempt {i+1}/{max_retries}). Retrying in 5s...", flush=True)
            time.sleep(5)
    
    # [Fallback] If DB failed, try loading from local CSV
    if df is None or df.empty:
        print("âš ï¸ DB Load failed. Attempting to load from local CSV (Fallback)...", flush=True)
        csv_path = 'cleaned_merged_export_trends.csv'
        if not os.path.exists(csv_path):
            parent_path = os.path.join('..', csv_path)
            if os.path.exists(parent_path):
                csv_path = parent_path
        
        if os.path.exists(csv_path):
             try:
                print(f"Fallback: Loading {csv_path}...", flush=True)
                df = pd.read_csv(csv_path, low_memory=False, dtype={'period': str})
                
                # Basic Preprocessing for CSV
                if 'period' in df.columns:
                    def convert_period(val):
                        try:
                            if pd.isna(val) or val == '': return ''
                            s = str(val).strip()
                            parts = s.split('.')
                            year = parts[0]
                            if len(parts) > 1:
                                month_part = parts[1]
                                if len(month_part) == 2: month = month_part
                                elif len(month_part) == 1: month = str(int(month_part) + 9).zfill(2)  # 1->10, 2->11, 3->12
                                else: month = str(month_part)[:2].zfill(2)
                            else: month = '01'
                            return f"{year}-{month}"
                        except: return ''
                    df['period_str'] = df['period'].apply(convert_period)
                    df = df.sort_values(by=['country_name', 'item_name', 'period_str'])
                
                numeric_targets = ['export_value', 'export_weight', 'unit_price', 'exchange_rate', 'gdp_level', 'cpi']
                for col in numeric_targets:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
                        
                print("Fallback CSV Loaded successfully. (Note: Growth Matrix not recalculated)", flush=True)
             except Exception as e:
                 print(f"Fallback CSV Load Failed: {e}", flush=True)

    if df is None or df.empty: 
        print("âŒ Final: Could not load data after retries. App will run with empty state.", flush=True)
        df = pd.DataFrame()
        growth_summary_df = pd.DataFrame()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Initialize empty first to prevent errors if requests come in before load
    global df, growth_summary_df
    df = pd.DataFrame()
    growth_summary_df = pd.DataFrame()

    print("ğŸš€ Server Starting... Triggering Background Data Load.", flush=True)
    
    # Start background thread for data loading
    # This prevents blocking the startup, so Readiness Probe can pass immediately.
    loader_thread = threading.Thread(target=load_data_background, daemon=True)
    loader_thread.start()

    yield
    print("Shutting down...", flush=True)

app = FastAPI(title="K-Food Export Analysis Engine", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

@app.get("/")
async def root():
    return {"message": "K-Food Analysis Engine (Visual Analytics Mode)", "status": "Ready"}

@app.get("/health/data")
async def health_data():
    """Check if data is loaded"""
    return {
        "data_loaded": not (df is None or df.empty),
        "rows": len(df) if df is not None else 0,
        "growth_matrix_rows": len(growth_summary_df) if growth_summary_df is not None else 0,
        "global_stats": {
            "sentiment": GLOBAL_MEAN_SENTIMENT,
            "rating": GLOBAL_MEAN_RATING
        }
    }

@app.get("/items")
async def get_items():
    if df is None or df.empty: return {"items": []}
    try:
        csv_items = df['item_name'].dropna().unique().tolist()
        ui_items = sorted(list(set([CSV_TO_UI_ITEM_MAPPING.get(i, i) for i in csv_items])))
        return {"items": ui_items}
    except: return {"items": []}

@app.get("/analyze")
async def analyze(country: str = Query(...), item: str = Query(...)):
    
    # [Debug] Log incoming request
    print(f"[Analyze] Request: country={country}, item={item}", flush=True)
    
    # 1. ë§¤í•‘ ë° ìœ íš¨ì„± ê²€ì‚¬
    country_name = REVERSE_MAPPING.get(country, country) # ì½”ë“œ(US) -> ì´ë¦„(ë¯¸êµ­)
    if country in COUNTRY_MAPPING: # ì…ë ¥ì´ í•œê¸€(ë¯¸êµ­)ì´ë©´ ì½”ë“œë¡œ ë³€í™˜
         country_code = COUNTRY_MAPPING[country]
         country_name = country
    else:
         country_code = country # ì…ë ¥ì´ ì½”ë“œ(US)ë©´ ê·¸ëŒ€ë¡œ
         
    csv_item_name = UI_TO_CSV_ITEM_MAPPING.get(item, item)
    print(f"[Analyze] Mapped: country_name={country_name}, country_code={country_code}, csv_item={csv_item_name}", flush=True)
    
    # ë°ì´í„° í•„í„°ë§
    filtered = df[
        (df['country_name'] == country_name) & 
        (df['item_name'] == csv_item_name)
    ].copy()
    
    print(f"[Analyze] Filtered rows: {len(filtered)}", flush=True)
    
    if filtered.empty or (filtered['export_value'].sum() == 0):
        print(f"[Analyze] No data found for {country_name} - {csv_item_name}", flush=True)
        return {"has_data": False}

    # ë‚ ì§œìˆœ ì •ë ¬ (period_str: 2022.01, 2022.1 ë“± í˜¼ìš© ëŒ€ì‘)
    if not filtered.empty:
        # 2022.1 -> 2022.01 ë³€í™˜ ë° ì •ê·œí™”
        def normalize_period(p):
            if not isinstance(p, str): return p
            parts = p.split('.')
            if len(parts) == 2:
                return f"{parts[0]}.{parts[1].zfill(2)}"
            return p
            
        filtered['period_str'] = filtered['period_str'].apply(normalize_period)
        filtered = filtered.sort_values('period_str')

    # ---------------------------------------------------------
    # Chart 1: Trend Stack (ìˆ˜ì¶œì•¡ + í™˜ìœ¨ ì¦ê°ë¥  + GDP ì¦ê°ë¥ )
    # ---------------------------------------------------------
    rows = 2
    titles = [f"ğŸ“Š {country_name} {item} ìˆ˜ì¶œì•¡ ì¶”ì´", f"ğŸ’± {country_name} í™˜ìœ¨ ì¦ê°ë¥  (%)"]
    if 'gdp_level' in filtered.columns:
        rows = 3
        titles.append(f"ğŸ“ˆ {country_name} GDP ì¦ê°ë¥  (MoM %)")
        
    fig_stack = make_subplots(rows=rows, cols=1, shared_xaxes=True, 
                              vertical_spacing=0.12, subplot_titles=titles)
                              
    # Row 1: Export Value (Bar + Color Gradient)
    export_values = filtered['export_value']
    fig_stack.add_trace(go.Bar(
        x=filtered['period_str'], y=export_values, name="ìˆ˜ì¶œì•¡ ($)",
        marker=dict(color=export_values, colorscale='Purples'),
        hovertemplate='%{x}<br>ìˆ˜ì¶œì•¡: $%{y:,.0f}<extra></extra>'
    ), row=1, col=1)
    
    # â˜… ìµœê³ ì /ìµœì €ì  annotation
    if len(filtered) >= 3:
        max_idx = export_values.idxmax()
        min_idx = export_values[export_values > 0].idxmin() if (export_values > 0).any() else export_values.idxmin()
        max_val = export_values[max_idx]
        min_val = export_values[min_idx]
        max_period = filtered.loc[max_idx, 'period_str']
        min_period = filtered.loc[min_idx, 'period_str']
        
        fig_stack.add_annotation(
            x=max_period, y=max_val, text=f"<b>ìµœê³ </b> ${max_val:,.0f}",
            showarrow=True, arrowhead=2, arrowcolor="#7c3aed",
            font=dict(color="#7c3aed", size=12, family="Arial Black"),
            bgcolor="rgba(124,58,237,0.1)", bordercolor="#7c3aed", borderwidth=1,
            borderpad=4, row=1, col=1
        )
        fig_stack.add_annotation(
            x=min_period, y=min_val, text=f"<b>ìµœì €</b> ${min_val:,.0f}",
            showarrow=True, arrowhead=2, arrowcolor="#ef4444",
            font=dict(color="#ef4444", size=11),
            bgcolor="rgba(239,68,68,0.1)", bordercolor="#ef4444", borderwidth=1,
            borderpad=4, row=1, col=1
        )
    
    # â˜… í‰ê· ì„  ì¶”ê°€
    avg_export = export_values.mean()
    fig_stack.add_hline(y=avg_export, line_dash="dash", line_color="#94a3b8", line_width=1,
                        annotation_text=f"í‰ê·  ${avg_export:,.0f}", annotation_position="top right",
                        annotation_font=dict(size=10, color="#94a3b8"), row=1, col=1)
    
    # â˜… YoY ì„±ì¥ë¥  ê³„ì‚° (ì¸ì‚¬ì´íŠ¸ìš©)
    trend_summary_parts = []
    if len(filtered) >= 12:
        recent_6m = export_values.tail(6).sum()
        prev_6m = export_values.iloc[-12:-6].sum()
        if prev_6m > 0:
            yoy_growth = ((recent_6m - prev_6m) / prev_6m) * 100
            if yoy_growth > 0:
                trend_summary_parts.append(f"ìµœê·¼ 6ê°œì›” ìˆ˜ì¶œì•¡ì´ ì „ê¸° ëŒ€ë¹„ +{yoy_growth:.1f}% ì„±ì¥í–ˆìŠµë‹ˆë‹¤ ğŸ“ˆ")
            else:
                trend_summary_parts.append(f"ìµœê·¼ 6ê°œì›” ìˆ˜ì¶œì•¡ì´ ì „ê¸° ëŒ€ë¹„ {yoy_growth:.1f}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤ ğŸ“‰")
    elif len(filtered) >= 2:
        last_val = export_values.iloc[-1]
        first_val = export_values.iloc[0]
        if first_val > 0:
            total_growth = ((last_val - first_val) / first_val) * 100
            trend_summary_parts.append(f"ì „ì²´ ê¸°ê°„ ìˆ˜ì¶œì•¡ì´ {total_growth:+.1f}% ë³€ë™í–ˆìŠµë‹ˆë‹¤")
    
    # Row 2: Exchange Rate ì¦ê°ë¥  (Bar with red/green)
    exchange_rate_pct = filtered['exchange_rate'].pct_change().fillna(0) * 100
    exchange_colors = ['#ef4444' if v < 0 else '#22c55e' for v in exchange_rate_pct]
    fig_stack.add_trace(go.Bar(
        x=filtered['period_str'], y=exchange_rate_pct.round(2), name="í™˜ìœ¨ ì¦ê°ë¥ ",
        marker=dict(color=exchange_colors),
        hovertemplate='%{x}<br>ì¦ê°ë¥ : %{y:.2f}%<extra></extra>'
    ), row=2, col=1)
    
    # â˜… í™˜ìœ¨-ìˆ˜ì¶œ ìƒê´€ê´€ê³„ ë¶„ì„
    try:
        corr_exchange = export_values.corr(filtered['exchange_rate'])
        if not np.isnan(corr_exchange):
            corr_label = "ê°•í•œ ì–‘ì˜ ìƒê´€" if corr_exchange > 0.5 else "ê°•í•œ ìŒì˜ ìƒê´€" if corr_exchange < -0.5 else "ì•½í•œ ìƒê´€"
            if abs(corr_exchange) > 0.3:
                trend_summary_parts.append(f"í™˜ìœ¨ê³¼ ìˆ˜ì¶œì•¡ì˜ ìƒê´€ê³„ìˆ˜: {corr_exchange:.2f} ({corr_label})")
    except:
        pass
    
    # Row 3: GDP ì¦ê°ë¥  ìˆ˜ì •
    if rows == 3:
        # [ìˆ˜ì •] í’ˆëª© í•„í„°ë§ê³¼ ë¬´ê´€í•˜ê²Œ í•´ë‹¹ êµ­ê°€ì˜ ì „ì²´ GDP ì‹œê³„ì—´ í™•ë³´ (2025ë…„ ëŠê¹€ ë°©ì§€)
        gdp_full = df[df['country_name'] == country_name][['period_str', 'gdp_level']].drop_duplicates('period_str').sort_values('period_str')
        
        # 1. ì¤‘ë³µ ê°’(ê³„ë‹¨)ì„ NaNìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•˜ì—¬ "ì "ìœ¼ë¡œ ë§Œë“¦
        # 0 ì œê±° ë° ì •ê·œí™”ëœ period_str ê¸°ë°˜ ì •ë ¬
        gdp_full['period_str'] = gdp_full['period_str'].apply(normalize_period)
        gdp_full = gdp_full.sort_values('period_str')
        
        gdp_series = gdp_full['gdp_level'].replace(to_replace=0, method='ffill')
        mask = gdp_series != gdp_series.shift(1)
        gdp_masked = gdp_series.where(mask)
        
        # 2. ì„ í˜• ë³´ê°„ (Linear Interpolation)ìœ¼ë¡œ "ì„ "ìœ¼ë¡œ ì´ìŒ -> ì›”ë³„ ë¶€ë“œëŸ¬ìš´ ì„±ì¥
        gdp_interpolated = gdp_masked.interpolate(method='linear', limit_direction='both')

        fig_stack.add_trace(go.Scatter(
            x=gdp_full['period_str'], 
            y=gdp_interpolated, 
            name="GDP",
            line=dict(color='#10b981', width=2, dash='dot'),
            hovertemplate='%{x}<br>GDP: %{y:,.0f}<extra></extra>'
        ), row=3, col=1)

    fig_stack.update_layout(
        height=600 if rows == 3 else 450, 
        template="plotly_white", 
        showlegend=False,
        margin=dict(l=40, r=20, t=60, b=40)
    )
    
    trend_insight = " | ".join(trend_summary_parts) if trend_summary_parts else f"{country_name} {item} ìˆ˜ì¶œ ì¶”ì´ë¥¼ í™•ì¸í•˜ì„¸ìš”"

    # ---------------------------------------------------------
    # Chart 2: Signal Map (Leading-Lagging)
    # ---------------------------------------------------------
    fig_signal = make_subplots(specs=[[{"secondary_y": True}]])
    
    common_trend_key = f"{country_code}_KFood_mean"
    
    # Helper to safely get value from dict
    def get_trend_val(row, key):
        td = row.get('trend_data', {})
        if not isinstance(td, dict): return None
        return td.get(key)

    # 1. ê³µí†µ ì„ í–‰ ì§€í‘œ: ì „ì²´ K-Food ê´€ì‹¬ë„ (Baseline)
    # Check if we have data for this key in the first row (as a sample)
    has_common = False
    first_trend_data = filtered.iloc[0].get('trend_data', {}) if not filtered.empty else {}
    if isinstance(first_trend_data, dict) and common_trend_key in first_trend_data:
        has_common = True
        
    if has_common:
        # Extract series
        y_common = filtered.apply(lambda r: get_trend_val(r, common_trend_key), axis=1)
        fig_signal.add_trace(go.Scatter(
            x=filtered['period_str'], y=y_common, 
            name="K-Food ì „ì²´ ê´€ì‹¬ë„",
            line=dict(color='#94a3b8', width=2, dash='dot'), # ì°¨ë¶„í•œ íšŒìƒ‰ ì ì„  (ê°€ë…ì„± í–¥ìƒ)
            opacity=0.8
        ), secondary_y=True)

    # 2. ê°œë³„ ì„ í–‰ ì§€í‘œ: 1:1 ë§¤í•‘ëœ í’ˆëª© ê´€ì‹¬ë„
    trend_kw = ITEM_TO_TREND_MAPPING.get(item)
    # KFoodì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€ë¡œ ê·¸ë¦¼
    if trend_kw and trend_kw != "KFood":
        specific_trend_key = f"{country_code}_{trend_kw}_mean"
        
        # Check existence
        has_specific = False
        if isinstance(first_trend_data, dict) and specific_trend_key in first_trend_data:
             has_specific = True
             
        if has_specific:
            y_specific = filtered.apply(lambda r: get_trend_val(r, specific_trend_key), axis=1)
            fig_signal.add_trace(go.Scatter(
                x=filtered['period_str'], y=y_specific, 
                name=f"í’ˆëª© ê´€ì‹¬ë„ ({trend_kw})",
                line=dict(color='#6366f1', width=3), # ì¸ë””ê³  ìƒ‰ìƒìœ¼ë¡œ ë³€ê²½ (ë„ˆë¬´ íŠ€ì§€ ì•Šê²Œ)
                mode='lines+markers'
            ), secondary_y=True)
            
    elif not has_common:
        # KFoodë„ ì—†ê³  ë§¤í•‘ë„ ì—†ì„ ë•Œë§Œ ì•„ë¬´ íŠ¸ë Œë“œë‚˜ í•˜ë‚˜ ì°¾ì•„ì„œ í‘œì‹œ (í´ë°±)
        # Find any key ending in _mean inside the first row's trend_data
        fallback_key = None
        if isinstance(first_trend_data, dict):
            for k in first_trend_data.keys():
                if k.startswith(f"{country_code}_") and k.endswith("_mean"):
                    fallback_key = k
                    break
        
        if fallback_key:
            y_fallback = filtered.apply(lambda r: get_trend_val(r, fallback_key), axis=1)
            fig_signal.add_trace(go.Scatter(
                x=filtered['period_str'], y=y_fallback, 
                name="ê´€ì‹¬ë„ (ê´€ë ¨ ë°ì´í„°)",
                line=dict(color='#f43f5e', width=3)
            ), secondary_y=True)

    # 3. í›„í–‰ ì§€í‘œ: ì‹¤ì (ìˆ˜ì¶œì•¡) - Bar Chart (ë°°ê²½ ì—­í• )
    fig_signal.add_trace(go.Bar(
        x=filtered['period_str'], y=filtered['export_value'], name="ìˆ˜ì¶œ ì‹¤ì  ($)",
        marker=dict(color='rgba(99, 102, 241, 0.3)'),
        hovertemplate='%{x}<br>ìˆ˜ì¶œì•¡: $%{y:,.0f}<extra></extra>'
    ), secondary_y=False)
    
    # â˜… ê´€ì‹¬ë„-ìˆ˜ì¶œ ìƒê´€ê³„ìˆ˜ ê³„ì‚° & í”¼í¬ annotation
    signal_summary_parts = []
    signal_corr_text = ""
    
    # ê´€ì‹¬ë„ ë°ì´í„° ì¶”ì¶œ (ê°€ì¥ ì¢‹ì€ ê±¸ë¡œ)
    trend_series = None
    trend_label = ""
    if trend_kw and trend_kw != "KFood":
        specific_key = f"{country_code}_{trend_kw}_mean"
        if isinstance(first_trend_data, dict) and specific_key in first_trend_data:
            trend_series = filtered.apply(lambda r: get_trend_val(r, specific_key), axis=1)
            trend_label = trend_kw
    if trend_series is None and has_common:
        trend_series = filtered.apply(lambda r: get_trend_val(r, common_trend_key), axis=1)
        trend_label = "K-Food"
    
    if trend_series is not None:
        trend_clean = trend_series.dropna()
        export_clean = filtered.loc[trend_clean.index, 'export_value']
        
        try:
            corr_val = trend_clean.astype(float).corr(export_clean.astype(float))
            if not np.isnan(corr_val):
                corr_strength = "ê°•í•œ" if abs(corr_val) > 0.5 else "ì¤‘ê°„" if abs(corr_val) > 0.3 else "ì•½í•œ"
                corr_direction = "ì–‘ì˜" if corr_val > 0 else "ìŒì˜"
                signal_corr_text = f" (r={corr_val:.2f})"
                signal_summary_parts.append(f"ê´€ì‹¬ë„ì™€ ìˆ˜ì¶œ ì‹¤ì ì˜ {corr_strength} {corr_direction} ìƒê´€ê´€ê³„ (r={corr_val:.2f})")
        except:
            pass
        
        # â˜… ê´€ì‹¬ë„ í”¼í¬ ìë™ annotation
        try:
            trend_numeric = trend_clean.astype(float)
            if len(trend_numeric) >= 4:
                mean_trend = trend_numeric.mean()
                std_trend = trend_numeric.std()
                threshold = mean_trend + std_trend * 1.2
                
                peaks = trend_numeric[trend_numeric > threshold]
                for peak_idx in peaks.index[:3]:  # ìµœëŒ€ 3ê°œ í”¼í¬ë§Œ
                    peak_val = trend_numeric[peak_idx]
                    peak_period = filtered.loc[peak_idx, 'period_str']
                    fig_signal.add_annotation(
                        x=peak_period, y=float(peak_val), text="ğŸ”¥ ê´€ì‹¬ ê¸‰ë“±",
                        showarrow=True, arrowhead=2, arrowcolor="#f43f5e",
                        font=dict(color="#f43f5e", size=10),
                        bgcolor="rgba(244,63,94,0.1)", bordercolor="#f43f5e", borderwidth=1,
                        borderpad=3, secondary_y=True
                    )
        except:
            pass
    
    if not signal_summary_parts:
        signal_summary_parts.append(f"{country_name}ì—ì„œì˜ {item} ê´€ì‹¬ë„ì™€ ìˆ˜ì¶œ ì‹¤ì  ì‹œì°¨ë¥¼ ë¹„êµí•©ë‹ˆë‹¤")
    
    signal_insight = " | ".join(signal_summary_parts)
    
    fig_signal.update_layout(
        template="plotly_white",
        height=420,
        legend=dict(orientation="h", y=1.12, x=0.5, xanchor='center'),
        margin=dict(l=50, r=50, t=30, b=40) # ì œëª© ì œê±°ì— ë”°ë¥¸ ì—¬ë°± ì¡°ì •
    )
    fig_signal.update_yaxes(title_text="ìˆ˜ì¶œì•¡ ($)", secondary_y=False, showgrid=False)
    fig_signal.update_yaxes(title_text="ê´€ì‹¬ë„ Index", secondary_y=True, showgrid=False)

    # ---------------------------------------------------------
    # Chart 3: Growth Matrix (Scatter Plot)
    # ---------------------------------------------------------
    country_matrix = growth_summary_df[growth_summary_df['country'] == country_code].copy()
    fig_scatter = go.Figure()
    growth_diagnosis = ""
    
    if not country_matrix.empty and not country_matrix[country_matrix['item_csv_name'] == csv_item_name].empty:
        country_matrix['ui_name'] = country_matrix['item_csv_name'].apply(lambda x: CSV_TO_UI_ITEM_MAPPING.get(x, x))
        
        curr = country_matrix[country_matrix['item_csv_name'] == csv_item_name]
        others = country_matrix[country_matrix['item_csv_name'] != csv_item_name]
        
        # â˜… Others â€” ìƒìœ„ 5ê°œì—ë§Œ ë¼ë²¨ í‘œì‹œ
        others_sorted = others.sort_values('weight_growth', ascending=False)
        top_others = others_sorted.head(5)
        rest_others = others_sorted.iloc[5:]
        
        # ë‚˜ë¨¸ì§€ í’ˆëª© (ë¼ë²¨ ì—†ìŒ)
        if not rest_others.empty:
            fig_scatter.add_trace(go.Scatter(
                x=np.clip(rest_others['weight_growth'], -150, 150), 
                y=np.clip(rest_others['price_growth'], -50, 50),
                mode='markers',
                marker=dict(size=8, color='#cbd5e1', opacity=0.3),
                text=rest_others['ui_name'], name="íƒ€ í’ˆëª©",
                hovertemplate="<b>%{text}</b><br>ì–‘ì : %{x}%<br>ì§ˆì : %{y}%"
            ))
        
        # ìƒìœ„ 5ê°œ í’ˆëª© (ë¼ë²¨ í‘œì‹œ)
        if not top_others.empty:
            fig_scatter.add_trace(go.Scatter(
                x=np.clip(top_others['weight_growth'], -150, 150), 
                y=np.clip(top_others['price_growth'], -50, 50),
                mode='markers+text',
                marker=dict(size=11, color='#94a3b8', opacity=0.6),
                text=top_others['ui_name'], textposition="top center",
                textfont=dict(size=10, color='#64748b'),
                name="ì£¼ìš” í’ˆëª©",
                hovertemplate="<b>%{text}</b><br>ì–‘ì : %{x}%<br>ì§ˆì : %{y}%"
            ))
        
        # â˜… Current â€” ring ë§ˆì»¤ íš¨ê³¼ (ì™¸ê³½ í° ì› + ë‚´ë¶€ ì›)
        curr_x_clamped = np.clip(curr['weight_growth'], -150, 150)
        curr_y_clamped = np.clip(curr['price_growth'], -50, 50)
        
        fig_scatter.add_trace(go.Scatter(
            x=curr_x_clamped, y=curr_y_clamped,
            mode='markers',
            marker=dict(size=35, color='rgba(244,63,94,0.15)', line=dict(width=3, color='#f43f5e')),
            showlegend=False, hoverinfo='skip'
        ))
        fig_scatter.add_trace(go.Scatter(
            x=curr_x_clamped, y=curr_y_clamped,
            mode='markers+text',
            marker=dict(size=20, color='#f43f5e', line=dict(width=2, color='white')),
            text=curr['ui_name'], textposition="top center",
            textfont=dict(size=15, color='#f43f5e', family="Arial Black"),
            name=item,
            hovertemplate="<b>%{text}</b> (í˜„ì¬)<br>ì–‘ì : %{x}%<br>ì§ˆì : %{y}%"
        ))
        
        # ì‚¬ë¶„ë©´ ì§„ë‹¨ ë©”ì‹œì§€ ìƒì„±
        curr_wg = curr['weight_growth'].values[0]
        curr_pg = curr['price_growth'].values[0]
        
        if curr_wg >= 0 and curr_pg >= 0:
            growth_diagnosis = f"ğŸŒŸ {item}: ê³ ë¶€ê°€ê°€ì¹˜ ì„±ì¥ ì¤‘! ë¬¼ëŸ‰(+{curr_wg:.1f}%)ê³¼ ë‹¨ê°€(+{curr_pg:.1f}%)ê°€ ëª¨ë‘ ìƒìŠ¹í•˜ê³  ìˆìŠµë‹ˆë‹¤"
        elif curr_wg < 0 and curr_pg >= 0:
            growth_diagnosis = f"âš ï¸ {item}: ë‹¨ê°€ëŠ” +{curr_pg:.1f}% ìƒìŠ¹í–ˆì§€ë§Œ ë¬¼ëŸ‰ì´ {curr_wg:.1f}% ê°ì†Œ ì¤‘. ì‹œì¥ ì¶•ì†Œ ì£¼ì˜"
        elif curr_wg < 0 and curr_pg < 0:
            growth_diagnosis = f"ğŸ”» {item}: ë¬¼ëŸ‰({curr_wg:.1f}%)ê³¼ ë‹¨ê°€({curr_pg:.1f}%) ëª¨ë‘ í•˜ë½. ì‹œì¥ ì¬ì§„ì… ì „ëµ í•„ìš”"
        else:
            growth_diagnosis = f"ğŸ“¦ {item}: ë¬¼ëŸ‰ì€ +{curr_wg:.1f}% ì¦ê°€í•˜ì§€ë§Œ ë‹¨ê°€ê°€ {curr_pg:.1f}% í•˜ë½. ê°€ê²© ê²½ìŸë ¥ ì „ëµ í™•ì¸ í•„ìš”"
        
        # Quadrant Lines
        fig_scatter.add_hline(y=0, line_dash="solid", line_color="#94a3b8", line_width=2)
        fig_scatter.add_vline(x=0, line_dash="solid", line_color="#94a3b8", line_width=2)
        
        # ì‚¬ë¶„ë©´ ë°°ê²½ ì‰ì´ë”© - ê³ ì • ìŠ¤ì¼€ì¼ ê¸°ë°˜ (150%, 50%)
        x_limit = 150
        y_limit = 50
        
        fig_scatter.add_shape(type="rect", x0=0, y0=0, x1=x_limit, y1=y_limit, fillcolor="rgba(16, 185, 129, 0.06)", line_width=0, layer="below")
        fig_scatter.add_shape(type="rect", x0=-x_limit, y0=0, x1=0, y1=y_limit, fillcolor="rgba(245, 158, 11, 0.06)", line_width=0, layer="below")
        fig_scatter.add_shape(type="rect", x0=-x_limit, y0=-y_limit, x1=0, y1=0, fillcolor="rgba(239, 68, 68, 0.06)", line_width=0, layer="below")
        fig_scatter.add_shape(type="rect", x0=0, y0=-y_limit, x1=x_limit, y1=0, fillcolor="rgba(59, 130, 246, 0.06)", line_width=0, layer="below")
        
        # 4ì‚¬ë¶„ë©´ ë¼ë²¨ â€” ê³ ì • ìœ„ì¹˜
        fig_scatter.add_annotation(x=x_limit*0.7, y=y_limit*0.85, text="ğŸŒŸ Premium<br>(ê³ ë¶€ê°€ê°€ì¹˜ ì„±ì¥)", showarrow=False, font=dict(color="#10b981", size=14, family="Arial Black"), xanchor="center", opacity=0.9)
        fig_scatter.add_annotation(x=-x_limit*0.7, y=y_limit*0.85, text="âš ï¸ ë‹¨ê°€ ìƒìŠ¹<br>(ë¬¼ëŸ‰ ê°ì†Œ ì£¼ì˜)", showarrow=False, font=dict(color="#f59e0b", size=14, family="Arial Black"), xanchor="center", opacity=0.9)
        fig_scatter.add_annotation(x=-x_limit*0.7, y=-y_limit*0.85, text="ğŸ”» ì „ë©´ ìœ„ì¶•<br>(ì¬ì§„ì… ì „ëµ í•„ìš”)", showarrow=False, font=dict(color="#ef4444", size=14, family="Arial Black"), xanchor="center", opacity=0.9)
        fig_scatter.add_annotation(x=x_limit*0.7, y=-y_limit*0.85, text="ğŸ“¦ Volume Driven<br>(ë°•ë¦¬ë‹¤ë§¤ ê²½ìŸ)", showarrow=False, font=dict(color="#3b82f6", size=14, family="Arial Black"), xanchor="center", opacity=0.9)
        
        fig_scatter.update_layout(
            title=f"ì„±ì¥ì˜ ì§ˆ â€” {item} in {country_name}",
            xaxis_title="ì–‘ì  ì„±ì¥ (ë¬¼ëŸ‰ ì¦ê°€ìœ¨ %)",
            yaxis_title="ì§ˆì  ì„±ì¥ (ë‹¨ê°€ ì¦ê°€ìœ¨ %)",
            template="plotly_white",
            height=520,
            showlegend=False,
            margin=dict(l=50, r=30, t=70, b=40),
            xaxis=dict(range=[-x_limit, x_limit], zeroline=False),
            yaxis=dict(range=[-y_limit, y_limit], zeroline=False)
        )
    else:
        # ë°ì´í„°ê°€ ë¶€ì¡±í•´ì„œ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ì„ ë•Œ ë¹ˆ ì°¨íŠ¸
        growth_diagnosis = f"âšª {item}ì˜ ì„±ì¥ ë§¤íŠ¸ë¦­ìŠ¤ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
        fig_scatter.update_layout(
            title="ì„±ì¥ì˜ ì§ˆ (ë°ì´í„° ë¶€ì¡±)",
            template="plotly_white", height=500
        )

    return {
        "country": country,
        "country_name": country_name,
        "item": item,
        "has_data": True,
        "charts": {
            "trend_stack": json.loads(fig_stack.to_json()),
            "signal_map": json.loads(fig_signal.to_json()),
            "growth_matrix": json.loads(fig_scatter.to_json())
        },
        "insights": {
            "trend_summary": trend_insight,
            "signal_summary": signal_insight,
            "growth_diagnosis": growth_diagnosis
        }
    }

def generate_business_insights(df):
    """
    ê²€ìƒ‰ëœ ë°ì´í„°(df)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ìš© ì‹¬í™” ì°¨íŠ¸ 4ì¢…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    charts = {}

    # [Chart 1] í‰ì  vs ì‹¤ì œ ê°ì„± ì ìˆ˜ ë¹„êµ (Review Reliability)
    if 'sentiment_score' in df.columns and 'rating' in df.columns:
        try:
            sentiment_by_rating = df.groupby('rating')['sentiment_score'].mean().reset_index()
            fig1 = px.bar(sentiment_by_rating, x='rating', y='sentiment_score',
                          title="í‰ì  ëŒ€ë¹„ ì‹¤ì œ ê°ì„± ì ìˆ˜ (ì§„ì •ì„± ë¶„ì„)",
                          labels={'rating': 'ë³„ì ', 'sentiment_score': 'í‰ê·  ê°ì„± ì ìˆ˜'},
                          color='sentiment_score', color_continuous_scale='Blues')
            fig1.update_layout(template="plotly_white")
            charts['sentiment_analysis'] = json.loads(fig1.to_json())
        except Exception as e:
            print(f"[Insights] Chart 1 Error: {e}")

    # [Chart 2] ì¬êµ¬ë§¤ ì˜ë„ ì €í•´ ìš”ì¸ ë¶„ì„ (Churn Drivers)
    metrics = ['quality_issues_semantic', 'delivery_issues_semantic', 'price_sensitive']
    metric_labels = {'quality_issues_semantic': 'í’ˆì§ˆ ì´ìŠˆ', 
                     'delivery_issues_semantic': 'ë°°ì†¡ ì´ìŠˆ', 
                     'price_sensitive': 'ê°€ê²© ë¯¼ê°ë„'}
    
    repurchase_data = []
    for m in metrics:
        if m in df.columns:
            try:
                # ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼(ì´ìŠˆ ë“±)ì€ "ì´ìŠˆ ìœ ë¬´"ë¡œ ë³€í™˜í•˜ì—¬ ê·¸ë£¹í™”
                if m in ['quality_issues_semantic', 'delivery_issues_semantic']:
                    def has_issue(x):
                        if isinstance(x, list): return len(x) > 0
                        if isinstance(x, str): return bool(x)
                        return False
                    condition = df[m].apply(has_issue)
                else:
                    condition = df[m].fillna(0).astype(bool)
                
                temp_df = pd.DataFrame({
                    'Condition': condition,
                    'repurchase_intent_hybrid': df['repurchase_intent_hybrid']
                })
                
                group = temp_df.groupby('Condition')['repurchase_intent_hybrid'].mean().reset_index()
                group.columns = ['Condition', 'Rate']
                group['Factor'] = metric_labels[m]
                
                # True/False ë§¤í•‘
                group['Condition'] = group['Condition'].map({True: 'ì´ìŠˆ ìˆìŒ', False: 'ì´ìŠˆ ì—†ìŒ'})
                repurchase_data.append(group)
            except Exception as e:
                print(f"[Insights] Chart 2 Loop Error ({m}): {e}")
    
    if repurchase_data:
        try:
            rep_df = pd.concat(repurchase_data)
            fig2 = px.bar(rep_df, x='Factor', y='Rate', color='Condition', barmode='group',
                          title="ì´ìŠˆë³„ ì¬êµ¬ë§¤ ì˜ë„ ë³€í™” (ì´íƒˆ ìš”ì¸ ë¶„ì„)",
                          labels={'Rate': 'ì¬êµ¬ë§¤ ì˜ë„ í™•ë¥ ', 'Factor': 'ì£¼ìš” ìš”ì¸'},
                          color_discrete_map={'ì´ìŠˆ ìˆìŒ': '#EF553B', 'ì´ìŠˆ ì—†ìŒ': '#636EFA'})
            fig2.update_layout(template="plotly_white")
            charts['repurchase_drivers'] = json.loads(fig2.to_json())
        except Exception as e:
            print(f"[Insights] Chart 2 Build Error: {e}")


    # [Chart 3] ì£¼ìš” ë¶ˆë§Œ ìœ í˜•ë³„ í‰ì  íƒ€ê²© (Rating Impact)
    if 'semantic_top_dimension' in df.columns and 'rating' in df.columns:
        try:
            # None ì œê±°
            valid_df = df.dropna(subset=['semantic_top_dimension'])
            if not valid_df.empty:
                top_issues = valid_df['semantic_top_dimension'].value_counts().head(5).index
                issue_ratings = valid_df[valid_df['semantic_top_dimension'].isin(top_issues)].groupby('semantic_top_dimension')['rating'].mean().reset_index().sort_values('rating')
                
                fig3 = px.bar(issue_ratings, x='rating', y='semantic_top_dimension', orientation='h',
                              title="ì£¼ìš” ì´ìŠˆ ìœ í˜•ë³„ í‰ê·  í‰ì  (ë¦¬ìŠ¤í¬ ìš”ì¸)",
                              labels={'rating': 'í‰ê·  ë³„ì ', 'semantic_top_dimension': 'ì´ìŠˆ ìœ í˜•'},
                              color='rating', color_continuous_scale='Reds_r')
                fig3.update_layout(template="plotly_white")
                charts['issue_impact'] = json.loads(fig3.to_json())
        except Exception as e:
            print(f"[Insights] Chart 3 Error: {e}")

    # [Chart 4] ê¸ì •ì  ì‹ê° í‚¤ì›Œë“œ TOP 10 (Texture Analysis)
    def safe_parse(x):
        try: 
            if isinstance(x, list): return x
            return ast.literal_eval(x)
        except: return []

    if 'texture_terms' in df.columns:
        try:
            temp_df = df.copy()
            temp_df['texture_list'] = temp_df['texture_terms'].apply(safe_parse)
            exploded = temp_df.explode('texture_list').dropna(subset=['texture_list'])
            
            if not exploded.empty:
                texture_stats = exploded.groupby('texture_list').agg(
                    count=('sentiment_score', 'count'),
                    avg_sentiment=('sentiment_score', 'mean')
                ).reset_index()
                
                # ë¹ˆë„ìˆ˜ 3íšŒ ì´ìƒì¸ ê²ƒ ì¤‘ (ë°ì´í„° ì ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 5->3 ì™„í™”)
                top_textures = texture_stats[texture_stats['count'] >= 3].sort_values('avg_sentiment', ascending=False).head(10)
                
                if not top_textures.empty:
                    fig4 = px.bar(top_textures, x='avg_sentiment', y='texture_list', orientation='h',
                                  title="ê³ ê°ì´ ì„ í˜¸í•˜ëŠ” ì‹ê° í‚¤ì›Œë“œ Top 10",
                                  labels={'avg_sentiment': 'ê°ì„± ì ìˆ˜', 'texture_list': 'ì‹ê° í‘œí˜„'},
                                  color='avg_sentiment', color_continuous_scale='Greens')
                    fig4.update_layout(template="plotly_white")
                    charts['texture_keywords'] = json.loads(fig4.to_json())
        except Exception as e:
            print(f"[Insights] Chart 4 Error: {e}")

    return charts

@app.get("/analyze/consumer")
async def analyze_consumer(item_id: str = Query(None, description="ASIN"), item_name: str = Query(None, description="ì œí’ˆëª…/í‚¤ì›Œë“œ")):
    
    # 0. í‚¤ì›Œë“œ ì¹˜í™˜ (ê²€ìƒ‰ëŸ‰ ë¶€ì¡± ì´ìŠˆ í•´ê²°)
    if item_name:
        # Gochujang ê´€ë ¨ í‚¤ì›Œë“œê°€ ë“¤ì–´ì˜¤ë©´ Kimchië¡œ ìš°íšŒí•˜ì—¬ í’ë¶€í•œ ë°ì´í„° ì œê³µ
        target_keywords = ['gochujang', 'red pepper paste', 'hot pepper paste', 'korean paste']
        if any(k in item_name.lower() for k in target_keywords):
            print(f"[Consumer] Remapping '{item_name}' to 'Kimchi' for sufficient data analysis.", flush=True)
            item_name = 'Kimchi'
    
    conn = get_db_connection()
    if not conn:
         return JSONResponse(status_code=500, content={"has_data": False, "message": "Database Connection Error"})

    try:
        # DBì—ì„œ ì§ì ‘ ì¡°íšŒ (Memory Efficient)
        if item_name:
            query = """
                SELECT * FROM amazon_reviews 
                WHERE title ILIKE %s OR cleaned_text ILIKE %s OR original_text ILIKE %s
            """
            search_pattern = f"%{item_name}%"
            filtered = pd.read_sql(query, conn, params=(search_pattern, search_pattern, search_pattern))
            
        elif item_id:
            query = "SELECT * FROM amazon_reviews WHERE asin = %s"
            filtered = pd.read_sql(query, conn, params=(item_id,))
        else:
            filtered = pd.DataFrame()
            
    except Exception as e:
        print(f"[Consumer] Data Fetch Error: {e}", flush=True)
        filtered = pd.DataFrame()
    finally:
        conn.close()

    if filtered.empty:
        print(f"[Consumer] No data found for conditions: item_id={item_id}, item_name={item_name}", flush=True)
        return {"has_data": False, "message": "í•´ë‹¹ ì¡°ê±´ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    print(f"[Consumer] Found {len(filtered)} rows", flush=True)

    try:
        # === [ì¤‘ìš”] ë°ì´í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ëŒ€ì²´ ë¡œì§ ===
        # í‰ì  ë°ì´í„° ìˆ«ì ë³€í™˜
        if 'rating' in filtered.columns:
            filtered['rating'] = pd.to_numeric(filtered['rating'], errors='coerce').fillna(3.0)

        # 1. ê°ì„± ì ìˆ˜ (sentiment_score ì—†ì„ ê²½ìš° í‰ì  ê¸°ë°˜ ìƒì„±)
        if 'sentiment_score' not in filtered.columns:
            if 'rating' in filtered.columns:
                # 1->0.0, 5->1.0 í˜•íƒœë¡œ ë§¤í•‘
                filtered['sentiment_score'] = (filtered['rating'] - 1) / 4
            else:
                filtered['sentiment_score'] = 0.5
    except Exception as e:
        print(f"í•„í„°ë§/ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {"has_data": False, "message": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}"}

    # 2. êµ¬ë§¤/ì¶”ì²œ ì˜ë„ (ë°ì´í„° ì—†ì„ ê²½ìš° ê³ í‰ì  ê¸°ë°˜ ì¶”ë¡ )
    if 'repurchase_intent_hybrid' not in filtered.columns:
         filtered['repurchase_intent_hybrid'] = filtered['rating'] >= 4
    if 'recommendation_intent_hybrid' not in filtered.columns:
         filtered['recommendation_intent_hybrid'] = filtered['rating'] >= 4
         
    # 3. í‚¤ì›Œë“œ ì»¬ëŸ¼ (ë°ì´í„° ì—†ì„ ê²½ìš° ë¹ˆ ê°’ ìƒì„±)
    for col in ['review_text_keywords', 'title_keywords', 'flavor_terms', 'price', 'quality_issues_semantic', 'delivery_issues_semantic']:
        if col not in filtered.columns:
            filtered[col] = None
            
    # 4. íŒŒìƒ ë³€ìˆ˜ ì´ˆê¸°í™” (DBì— ì—†ê±°ë‚˜ ê³„ì‚°ë˜ì§€ ì•Šì€ ê²½ìš°)
    required_cols = ['value_perception_hybrid', 'price_sensitive', 'sensory_conflict']
    for col in required_cols:
         if col not in filtered.columns:
             filtered[col] = 0.5 if col == 'value_perception_hybrid' else (0.0 if col == 'price_sensitive' else False)

    total_count = filtered.shape[0]
    
    # =========================================================
    # 2. ì‹œì¥ ê°ì„± ë° ì£¼ìš” ì ìˆ˜ (ìƒëŒ€ì  ì§€í‘œë¡œ ì „ë©´ êµì²´)
    # =========================================================
    try:
        # [DIAGNOSTIC] Rating & Sentiment Distribution Log
        r_mean = filtered['rating'].mean()
        r_std = filtered['rating'].std()
        r_min = filtered['rating'].min()
        r_max = filtered['rating'].max()
        s_mean = filtered['sentiment_score'].mean()
        s_std = filtered['sentiment_score'].std()
        
        print(f"[Consumer-Diag] Total: {total_count}, Rating: mean={r_mean:.2f}, std={r_std:.2f}, min={r_min}, max={r_max}", flush=True)
        print(f"[Consumer-Diag] Sentiment: mean={s_mean:.2f}, std={s_std:.2f}", flush=True)

        # [Self-Healing] Detect invalid ratings (e.g., all 3.0 or 0 variance despite sentiment variance)
        # If rating variance is near 0 but sentiment variance is healthy, backfill rating from sentiment.
        is_rating_flat = (pd.isna(r_std) or r_std < 0.1) and (abs(r_mean - 3.0) < 0.1)
        is_sentiment_active = (not pd.isna(s_std) and s_std > 0.1)
        
        if is_rating_flat and is_sentiment_active:
            print("[Consumer] ğŸš¨ Detected abnormal ratings (all ~3.0). Attempting SELF-HEALING from sentiment_score...", flush=True)
            # Formula: Rating = Sentiment * 4 + 1 (Approximate mapping)
            # 0.0 -> 1.0, 0.5 -> 3.0, 1.0 -> 5.0
            filtered['rating'] = filtered['sentiment_score'] * 4 + 1
            # Recalculate stats
            r_mean = filtered['rating'].mean()
            print(f"[Consumer] Healed Rating Mean: {r_mean:.2f}", flush=True)

        # 1. Impact Score (Rating Lift)
        avg_rating = r_mean
        if pd.isna(avg_rating): avg_rating = 3.0
        item_impact_score = round(avg_rating - 3.0, 2)
        
        # 2. Relative Sentiment Z-Score
        target_mean_sent = s_mean
        if pd.isna(target_mean_sent): target_mean_sent = 0.5
        
        if GLOBAL_STD_SENTIMENT > 0:
            sentiment_z_score = round((target_mean_sent - GLOBAL_MEAN_SENTIMENT) / GLOBAL_STD_SENTIMENT, 2)
        else:
            sentiment_z_score = 0.0
            
        # 3. Satisfaction Index (Likelihood Ratio)
        target_five_star_ratio = (filtered['rating'] >= 4.5).mean() # 4.5 ì´ìƒì„ 5ì ìœ¼ë¡œ ê°„ì£¼ (Healed data may be float)
        if pd.isna(target_five_star_ratio): target_five_star_ratio = 0.0
        satisfaction_index = round(target_five_star_ratio / 0.2, 2)
        
        # ìš”ì•½ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        metrics = {
            "impact_score": item_impact_score,
            "sentiment_z_score": sentiment_z_score,
            "satisfaction_index": satisfaction_index,
            "total_reviews": total_count
        }
    except Exception as e:
        print(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics = {
            "impact_score": 0, "sentiment_z_score": 0, "satisfaction_index": 0, "total_reviews": total_count
        }
    
    # =========================================================
    # 3. ìƒì„¸ ë¶„ì„: Bigram ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ì„ (ê°€ë³€ ì„ê³„ê°’ ì ìš©)
    # =========================================================
    
    # ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ê°€ë³€ íŒŒë¼ë¯¸í„° ê²°ì •
    is_small_sample = total_count < 50
    adj_priority_val = not is_small_sample # 50ê°œ ë¯¸ë§Œì´ë©´ False (ëª¨ë“  í‚¤ì›Œë“œ í—ˆìš©)
    min_df_val = 1 if is_small_sample else 2 # 50ê°œ ë¯¸ë§Œì´ë©´ 1ë²ˆë§Œ ë‚˜ì™€ë„ ì¶”ì¶œ
    impact_threshold_val = 0.0 if is_small_sample else 0.1 # 50ê°œ ë¯¸ë§Œì´ë©´ ëª¨ë“  ì°¨ì´ ë…¸ì¶œ, ê·¸ ì™¸ì—” 0.1ë¡œ ì™„í™”
    
    # Bigram ì¶”ì¶œ ë° Impact Score, Positivity Rate ê³„ì‚°
    keywords_analysis = []
    diverging_keywords = {"negative": [], "positive": []}
    
    try:
        if 'cleaned_text' in filtered.columns and 'original_text' in filtered.columns:
            keywords_analysis = extract_bigrams_with_metrics(
                texts=filtered['cleaned_text'],
                ratings=filtered['rating'],
                original_texts=filtered['original_text'],
                top_n=20,
                adj_priority=adj_priority_val,
                min_df=min_df_val
            )
        
        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ë¶„ë¦¬ (ê°€ë³€ ì„ê³„ê°’ ê¸°ì¤€)
        diverging_keywords = get_diverging_keywords(
            keywords_analysis, 
            top_n=8, 
            threshold=impact_threshold_val
        )

        # [Logic Fix] ê¸ì • í‚¤ì›Œë“œê°€ ì—†ì§€ë§Œ ë¶„ì„ëœ í‚¤ì›Œë“œ ì¤‘ Impact Score ì–‘ìˆ˜ì¸ ê²ƒì´ ìˆë‹¤ë©´ "Soft Fallback"
        if not diverging_keywords["positive"] and keywords_analysis:
            # 0.02 ì´ìƒì¸ ê²ƒë“¤ì„ ì°¾ì•„ì„œ ì¶”ê°€ (Impact Scoreê°€ ì•„ì£¼ ë¯¸ì„¸í•˜ê²Œë¼ë„ ì–‘ìˆ˜ì¸ ê²ƒ)
            soft_positives = [k for k in keywords_analysis if k['impact_score'] > 0.02]
            if soft_positives:
                print(f"[Consumer] Soft Fallback: Found {len(soft_positives)} positive keywords (0.02 < score < {impact_threshold_val})", flush=True)
                diverging_keywords["positive"] = sorted(
                    soft_positives[:8],
                    key=lambda x: -x["impact_score"]
                )
        
        # â˜… ê¸ì • í‚¤ì›Œë“œ ë³´ì™„: impact_score ë°©ì‹ìœ¼ë¡œ ê¸ì •ì´ ì•ˆ ë‚˜ì˜¬ ë•Œ
        #   -> 4-5ì  ë¦¬ë·°ì—ì„œë§Œ ë³„ë„ Bigram ì¶”ì¶œí•˜ì—¬ ì±„ì›€
        if not diverging_keywords["positive"] and 'cleaned_text' in filtered.columns and 'original_text' in filtered.columns:
            print(f"[Consumer] No positive keywords from impact_score. Extracting from high-rated reviews (4-5â˜…)...", flush=True)
            pos_reviews = filtered[filtered['rating'] >= 4]
            
            if len(pos_reviews) >= 1:  # ê¸ì • ë¦¬ë·°ê°€ 1ê°œë¼ë„ ìˆìœ¼ë©´ ì¶”ì¶œ ì‹œë„ (ê¸°ì¡´ 3ê°œ -> 1ê°œ ì™„í™”)
                pos_min_df = 1 if len(pos_reviews) < 30 else 2
                pos_keywords_analysis = extract_bigrams_with_metrics(
                    texts=pos_reviews['cleaned_text'],
                    ratings=pos_reviews['rating'],
                    original_texts=pos_reviews['original_text'],
                    top_n=10,
                    adj_priority=False,  # ê¸ì •ì—ì„œëŠ” í˜•ìš©ì‚¬ í•„í„° í•´ì œ
                    min_df=pos_min_df
                )
                
                # ë¶€ì •ì— ì´ë¯¸ ë‚˜ì˜¨ í‚¤ì›Œë“œëŠ” ì œì™¸ (ì¤‘ë³µ ë°©ì§€)
                neg_keyword_set = {k["keyword"] for k in diverging_keywords["negative"]}
                pos_unique = [k for k in pos_keywords_analysis if k["keyword"] not in neg_keyword_set]
                
                # ìƒìœ„ 8ê°œë¥¼ ê¸ì • í‚¤ì›Œë“œë¡œ ì„¤ì •
                diverging_keywords["positive"] = sorted(
                    pos_unique[:8],
                    key=lambda x: -x["impact_score"]
                )
                print(f"[Consumer] Found {len(diverging_keywords['positive'])} positive keywords from high-rated reviews.", flush=True)
            
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    # =========================================================
    # 3-1. ì „ëµ ì¸ì‚¬ì´íŠ¸ ìë™ ìƒì„± (Critical Issue / Winning Point / Niche Opportunity)
    # =========================================================
    insights_data = {"critical_issue": None, "winning_point": None, "niche_opportunity": None}
    try:
        neg_reviews = filtered[filtered['rating'] <= 2]
        pos_reviews = filtered[filtered['rating'] >= 4]
        
        # --- ğŸš¨ Critical Issue: ë¶€ì • ë¦¬ë·°ì—ì„œ ê°€ì¥ ë§ì´ ì–¸ê¸‰ë˜ëŠ” Pain Point ---
        if not neg_reviews.empty and 'cleaned_text' in filtered.columns:
            neg_cleaned = neg_reviews['cleaned_text'].apply(remove_pos_tags).fillna('')
            try:
                neg_vec = CountVectorizer(ngram_range=(1, 2), min_df=1, max_features=500, stop_words='english', token_pattern=r'\b[a-zA-Z]{3,}\b')
                neg_matrix = neg_vec.fit_transform(neg_cleaned)
                neg_words_raw = neg_vec.get_feature_names_out()
                neg_counts_raw = neg_matrix.sum(axis=0).A1
                
                # ë²”ìš© ë‹¨ì–´ í•„í„°ë§
                neg_terms_filtered = []
                for i, word in enumerate(neg_words_raw):
                    if not is_generic_term(word):
                        neg_terms_filtered.append((word, int(neg_counts_raw[i])))
                
                neg_terms_filtered.sort(key=lambda x: -x[1])
                top_neg_terms = neg_terms_filtered[:5]
                
                neg_pct = round(len(neg_reviews) / total_count * 100, 1) if total_count > 0 else 0
                top_term = top_neg_terms[0][0] if top_neg_terms else "N/A"
                top_term_count = top_neg_terms[0][1] if top_neg_terms else 0
                
                # quality_issues_semantic ë¶„ì„ ì¶”ê°€
                quality_context = ""
                if 'quality_issues_semantic' in neg_reviews.columns:
                    qi_exploded = neg_reviews.explode('quality_issues_semantic')
                    qi_counts = qi_exploded['quality_issues_semantic'].dropna().value_counts()
                    if not qi_counts.empty:
                        top_qi = qi_counts.head(3).index.tolist()
                        quality_context = f" ì£¼ìš” í’ˆì§ˆ ì´ìŠˆ: {', '.join(top_qi)}"
                
                insights_data["critical_issue"] = {
                    "title": f"'{top_term}' ê´€ë ¨ ë¶ˆë§Œì´ ê°€ì¥ ì‹¬ê°í•©ë‹ˆë‹¤",
                    "description": f"ë¶€ì • ë¦¬ë·°(1-2ì )ì˜ {neg_pct}%ì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "data_evidence": f"ë¶€ì • ë¦¬ë·° {len(neg_reviews)}ê±´ ì¤‘ '{top_term}' {top_term_count}íšŒ ì–¸ê¸‰.{quality_context}",
                    "action_item": f"'{top_term}' ë¬¸ì œ í•´ê²°ì´ ìµœìš°ì„  ê³¼ì œì…ë‹ˆë‹¤. ìƒì„¸í˜ì´ì§€ì— ê°œì„  ì‚¬í•­ì„ ëª…ì‹œí•˜ì„¸ìš”.",
                    "top_terms": [{'term': t, 'count': c} for t, c in top_neg_terms]
                }
            except Exception as e:
                print(f"[Insight] Critical Issue extraction error: {e}", flush=True)
        
        # --- ğŸ‘ Winning Point: ê¸ì • ë¦¬ë·°ì—ì„œë§Œ ë‘ë“œëŸ¬ì§€ëŠ” í‚¤ì›Œë“œ ---
        if not pos_reviews.empty and 'cleaned_text' in filtered.columns:
            pos_cleaned = pos_reviews['cleaned_text'].apply(remove_pos_tags).fillna('')
            try:
                pos_vec = CountVectorizer(ngram_range=(1, 2), min_df=1, max_features=500, stop_words='english', token_pattern=r'\b[a-zA-Z]{3,}\b')
                pos_matrix = pos_vec.fit_transform(pos_cleaned)
                pos_words = pos_vec.get_feature_names_out()
                pos_counts_arr = pos_matrix.sum(axis=0).A1
                
                # ë¶€ì •ì—ì„œì˜ ë¹ˆë„ ê³„ì‚°
                neg_cleaned_all = neg_reviews['cleaned_text'].apply(remove_pos_tags).fillna('') if not neg_reviews.empty else pd.Series(dtype=str)
                neg_freq_map = {}
                if not neg_cleaned_all.empty:
                    try:
                        neg_vec2 = CountVectorizer(ngram_range=(1, 2), min_df=1, max_features=500, stop_words='english', token_pattern=r'\b[a-zA-Z]{3,}\b')
                        neg_matrix2 = neg_vec2.fit_transform(neg_cleaned_all)
                        neg_words2 = neg_vec2.get_feature_names_out()
                        neg_counts2 = neg_matrix2.sum(axis=0).A1
                        neg_freq_map = dict(zip(neg_words2, neg_counts2))
                    except:
                        pass
                
                # ê¸ì • ì „ìš© gapì´ í° í‚¤ì›Œë“œ ì°¾ê¸°
                gap_scores = []
                for i, word in enumerate(pos_words):
                    if is_generic_term(word):
                        continue
                    pos_freq = int(pos_counts_arr[i])
                    neg_freq = neg_freq_map.get(word, 0)
                    # ê¸ì •ì—ì„œì˜ ë¹„ìœ¨ - ë¶€ì •ì—ì„œì˜ ë¹„ìœ¨
                    pos_rate = pos_freq / len(pos_reviews) if len(pos_reviews) > 0 else 0
                    neg_rate = neg_freq / len(neg_reviews) if len(neg_reviews) > 0 else 0
                    gap = pos_rate - neg_rate
                    if pos_freq >= 2 and gap > 0:
                        gap_scores.append({'term': word, 'pos_freq': pos_freq, 'neg_freq': int(neg_freq), 'gap': round(gap, 3)})
                
                gap_scores.sort(key=lambda x: -x['gap'])
                top_winning = gap_scores[:5]
                
                if top_winning:
                    best = top_winning[0]
                    insights_data["winning_point"] = {
                        "title": f"ì†Œë¹„ìëŠ” '{best['term']}'ì— ì—´ê´‘í•˜ê³  ìˆìŠµë‹ˆë‹¤",
                        "description": f"ê¸ì • ë¦¬ë·°ì—ì„œ '{best['term']}'ì˜ ì–¸ê¸‰ ë¹„ìœ¨ì´ ë¶€ì • ë¦¬ë·° ëŒ€ë¹„ ì••ë„ì ìœ¼ë¡œ ë†’ìŠµë‹ˆë‹¤.",
                        "data_evidence": f"ê¸ì • ë¦¬ë·° {best['pos_freq']}íšŒ vs ë¶€ì • ë¦¬ë·° {best['neg_freq']}íšŒ ì–¸ê¸‰.",
                        "marketing_msg": f"'{best['term']}'ì„(ë¥¼) ë©”ì¸ ì¹´í”¼ë¡œ í™œìš©í•˜ì„¸ìš”.",
                        "top_terms": top_winning
                    }
            except Exception as e:
                print(f"[Insight] Winning Point extraction error: {e}", flush=True)
        
        # --- ğŸ’¡ Niche Opportunity: ì–¸ê¸‰ëŸ‰ ì ì§€ë§Œ ë§Œì¡±ë„ ë†’ì€ í‚¤ì›Œë“œ ---
        if keywords_analysis:
            median_mention = np.median([k['mention_count'] for k in keywords_analysis]) if keywords_analysis else 5
            niche_candidates = [
                k for k in keywords_analysis 
                if k['mention_count'] <= median_mention and k['impact_score'] > 0.3
            ]
            niche_candidates.sort(key=lambda x: -x['impact_score'])
            
            if niche_candidates:
                # ë²”ìš© ë‹¨ì–´ ì œì™¸ í•„í„°ë§ (ì´ë¯¸ keywords_analysisì—ì„œ ê±¸ë ¸ì„ ìˆ˜ ìˆì§€ë§Œ í•œ ë²ˆ ë” í™•ì¸)
                niche_candidates = [n for n in niche_candidates if not is_generic_term(n['keyword'])]
                
                if niche_candidates:
                    best_niche = niche_candidates[0]
                    avg_rating_niche = round(best_niche['impact_score'] + 3.0, 1)
                    insights_data["niche_opportunity"] = {
                        "title": f"'{best_niche['keyword']}' ê´€ë ¨ ì ì¬ ìˆ˜ìš”ê°€ ê°ì§€ë©ë‹ˆë‹¤",
                        "description": f"ì–¸ê¸‰ëŸ‰ì€ {best_niche['mention_count']}íšŒë¡œ ì ì§€ë§Œ, ì–¸ê¸‰ ì‹œ í‰ê·  ë³„ì ì´ {avg_rating_niche}ì ìœ¼ë¡œ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.",
                        "data_evidence": f"Impact Score: +{best_niche['impact_score']}, ë§Œì¡±ë„ ì§€ìˆ˜: {best_niche.get('satisfaction_index', 'N/A')}",
                        "top_terms": [{'term': k['keyword'], 'impact': k['impact_score'], 'mentions': k['mention_count']} for k in niche_candidates[:5]]
                    }
    except Exception as e:
        print(f"[Insight] Insight generation error: {e}", flush=True)
    
    # =========================================================
    # 3-2. Sentiment Gap Analysis Chart (ê¸ì • vs ë¶€ì • ë¹ˆë„ ë¹„êµ)
    # =========================================================
    fig_sentiment_gap = go.Figure()
    try:
        # ìƒìœ„ í‚¤ì›Œë“œì— ëŒ€í•´ ê¸ì •/ë¶€ì • ë¦¬ë·°ë³„ ë¹ˆë„ ê³„ì‚°
        gap_keywords = sorted(keywords_analysis, key=lambda x: -x['mention_count'])[:12]
        if gap_keywords and 'cleaned_text' in filtered.columns:
            pos_reviews_text = filtered[filtered['rating'] >= 4]['cleaned_text'].apply(remove_pos_tags).fillna('')
            neg_reviews_text = filtered[filtered['rating'] <= 2]['cleaned_text'].apply(remove_pos_tags).fillna('')
            
            kw_names = []
            pos_freqs = []
            neg_freqs = []
            
            for kw in gap_keywords:
                keyword = kw['keyword']
                p_count = pos_reviews_text.str.contains(keyword, case=False, na=False, regex=False).sum() if not pos_reviews_text.empty else 0
                n_count = neg_reviews_text.str.contains(keyword, case=False, na=False, regex=False).sum() if not neg_reviews_text.empty else 0
                kw_names.append(keyword)
                pos_freqs.append(int(p_count))
                neg_freqs.append(int(n_count))
            
            fig_sentiment_gap.add_trace(go.Bar(
                name='ê¸ì • ë¦¬ë·° (4-5ì )', x=kw_names, y=pos_freqs,
                marker_color='#22c55e',
                hovertemplate='<b>%{x}</b><br>ê¸ì • ë¦¬ë·° ì–¸ê¸‰: %{y}íšŒ<extra></extra>'
            ))
            fig_sentiment_gap.add_trace(go.Bar(
                name='ë¶€ì • ë¦¬ë·° (1-2ì )', x=kw_names, y=neg_freqs,
                marker_color='#ef4444',
                hovertemplate='<b>%{x}</b><br>ë¶€ì • ë¦¬ë·° ì–¸ê¸‰: %{y}íšŒ<extra></extra>'
            ))
    except Exception as e:
        print(f"[Chart] Sentiment Gap error: {e}", flush=True)
    
    fig_sentiment_gap.update_layout(
        title="í‚¤ì›Œë“œ ê°ì„± ì°¨ì´ ë¶„ì„ (Sentiment Gap)",
        xaxis_title="í‚¤ì›Œë“œ",
        yaxis_title="ì–¸ê¸‰ ë¹ˆë„",
        barmode='group',
        template='plotly_white',
        height=450,
        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1)
    )
    
    # =========================================================
    # 3-3. Keyword-Rating Correlation Chart (í‚¤ì›Œë“œë³„ í‰ê·  ë³„ì )
    # =========================================================
    fig_keyword_rating = go.Figure()
    try:
        rating_kw_data = sorted(keywords_analysis, key=lambda x: -x['mention_count'])[:15]
        if rating_kw_data:
            kw_labels = [k['keyword'] for k in rating_kw_data]
            kw_avg_ratings = [round(k['impact_score'] + 3.0, 2) for k in rating_kw_data]  # impact_score = avg - 3.0
            kw_colors = ['#22c55e' if r >= 4.0 else '#f59e0b' if r >= 3.0 else '#ef4444' for r in kw_avg_ratings]
            
            fig_keyword_rating.add_trace(go.Bar(
                y=kw_labels[::-1],
                x=kw_avg_ratings[::-1],
                orientation='h',
                marker_color=kw_colors[::-1],
                text=[f'{r:.1f}ì ' for r in kw_avg_ratings[::-1]],
                textposition='outside',
                hovertemplate='<b>%{y}</b><br>í‰ê·  ë³„ì : %{x}ì <extra></extra>'
            ))
            
            # ì „ì²´ í‰ê·  ê¸°ì¤€ì„ 
            overall_avg = round(filtered['rating'].mean(), 2) if 'rating' in filtered.columns else 3.0
            fig_keyword_rating.add_vline(
                x=overall_avg, line_dash='dash', line_color='#6366f1', line_width=2,
                annotation_text=f'ì „ì²´ í‰ê· : {overall_avg}ì ', annotation_position='top right'
            )
    except Exception as e:
        print(f"[Chart] Keyword-Rating Correlation error: {e}", flush=True)
    
    fig_keyword_rating.update_layout(
        title="í‚¤ì›Œë“œ-ë³„ì  ìƒê´€ê´€ê³„ (Keyword-Rating Correlation)",
        xaxis_title="í‰ê·  ë³„ì ",
        yaxis_title="í‚¤ì›Œë“œ",
        template='plotly_white',
        height=500,
        xaxis=dict(range=[1, 5.5]),
        showlegend=False
    )
    
    # =========================================================
    # =========================================================
    # 4. Diverging Bar Chart (ê°ì„± ì˜í–¥ë„ ì‹œê°í™”)
    # =========================================================
    
    # ë¶€ì • í‚¤ì›Œë“œ (Impact Score < 0)
    neg_keywords = diverging_keywords["negative"]
    pos_keywords = diverging_keywords["positive"]
    
    # Diverging Bar Chart ìƒì„± (xì¶• ê¸°ì¤€ 0)
    fig_diverging = go.Figure()
    
    # ë¶€ì • ì˜í–¥ í‚¤ì›Œë“œ (ì™¼ìª½, ë¹¨ê°„ìƒ‰)
    if neg_keywords:
        fig_diverging.add_trace(go.Bar(
            y=[k["keyword"] for k in neg_keywords],
            x=[k["impact_score"] for k in neg_keywords],
            orientation='h',
            name='ë¶€ì • ì˜í–¥',
            marker_color='#ef4444',
            text=[f'SI: {k["satisfaction_index"]}' for k in neg_keywords],
            textposition='inside',
            hovertemplate='<b>%{y}</b><br>ê°ì„± ì˜í–¥ë„: %{x}<br>ë§Œì¡±ë„ ì§€ìˆ˜: %{text}<extra></extra>'
        ))
    
    # ê¸ì • ì˜í–¥ í‚¤ì›Œë“œ (ì˜¤ë¥¸ìª½, ë…¹ìƒ‰)
    if pos_keywords:
        fig_diverging.add_trace(go.Bar(
            y=[k["keyword"] for k in pos_keywords],
            x=[k["impact_score"] for k in pos_keywords],
            orientation='h',
            name='ê¸ì • ì˜í–¥',
            marker_color='#22c55e',
            text=[f'SI: {k["satisfaction_index"]}' for k in pos_keywords],
            textposition='inside',
            hovertemplate='<b>%{y}</b><br>ê°ì„± ì˜í–¥ë„: %{x}<br>ë§Œì¡±ë„ ì§€ìˆ˜: %{text}<extra></extra>'
        ))
    
    fig_diverging.update_layout(
        title="í‚¤ì›Œë“œë³„ ê°ì„± ì˜í–¥ë„ (Impact Score)",
        xaxis_title="ê°ì„± ì˜í–¥ë„ (Impact Score: 0 = í‰ê· )",
        yaxis_title="í‚¤ì›Œë“œ (Bigram)",
        template="plotly_white",
        height=500,
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        xaxis=dict(zeroline=True, zerolinewidth=2, zerolinecolor='#64748b'),
        barmode='relative'
    )
    
    # =========================================================
    # 5. Satisfaction Index Chart (ë§Œì¡±ë„ í™•ë¥  ì§€ìˆ˜ ì‹œê°í™”)
    # =========================================================
    
    # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
    top_keywords_for_si = sorted(keywords_analysis, key=lambda x: -x["mention_count"])[:10]
    
    fig_positivity = go.Figure()
    if top_keywords_for_si:
        fig_positivity.add_trace(go.Bar(
            x=[k["keyword"] for k in top_keywords_for_si],
            y=[k["satisfaction_index"] for k in top_keywords_for_si],
            marker_color=[
                '#22c55e' if k["satisfaction_index"] >= 1.2 else '#f59e0b' if k["satisfaction_index"] >= 0.8 else '#ef4444'
                for k in top_keywords_for_si
            ],
            text=[f'{k["satisfaction_index"]}' for k in top_keywords_for_si],
            textposition='outside',
            hovertemplate='<b>%{x}</b><br>ë§Œì¡±ë„ ì§€ìˆ˜: %{y}<br>ì–¸ê¸‰ íšŸìˆ˜: %{customdata}<extra></extra>',
            customdata=[k["mention_count"] for k in top_keywords_for_si]
        ))
        
        # ê¸°ì¤€ì„  1.0 ì¶”ê°€
        fig_positivity.add_shape(
            type="line",
            x0=-0.5, y0=1.0, x1=len(top_keywords_for_si)-0.5, y1=1.0,
            line=dict(color="Red", width=2, dash="dash"),
        )
    
    fig_positivity.update_layout(
        title="í‚¤ì›Œë“œë³„ ë§Œì¡±ë„ í™•ë¥  ì§€ìˆ˜ (Satisfaction Index)",
        xaxis_title="í‚¤ì›Œë“œ (Bigram)",
        yaxis_title="Index (ê¸°ì¤€ 1.0)",
        template="plotly_white",
        height=400,
        yaxis=dict(rangemode='tozero') 
    )

    # =========================================================
    # 6. Advanced Consumer Experience Metrics
    # =========================================================

    # NSS (Net Sentiment Score) ê³„ì‚°
    pos_count = filtered[filtered['sentiment_score'] >= 0.75].shape[0]
    neg_count = filtered[filtered['sentiment_score'] <= 0.25].shape[0]
    nss_score = ((pos_count - neg_count) / total_count * 100) if total_count > 0 else 0
    
    # CAS (Customer Advocacy Score)
    advocates = filtered[
        (filtered['repurchase_intent_hybrid'] == True) & 
        (filtered['recommendation_intent_hybrid'] == True)
    ].shape[0]
    cas_score = (advocates / total_count) if total_count > 0 else 0
    
    # NSS ê²Œì´ì§€ ì°¨íŠ¸
    fig_nss = go.Figure(go.Indicator(
        mode = "gauge+number",
        value = nss_score,
        title = {'text': "NSS (ìˆœ ì •ì„œ ì ìˆ˜)"},
        gauge = {
            'axis': {'range': [-100, 100]},
            'bar': {'color': "darkblue"},
            'steps' : [
                {'range': [-100, -30], 'color': "#ff4d4f"},
                {'range': [-30, 30], 'color': "#faad14"},
                {'range': [30, 100], 'color': "#52c41a"}
            ],
            'threshold' : {'line': {'color': "black", 'width': 4}, 'thickness': 0.75, 'value': nss_score}
        }
    ))
    fig_nss.update_layout(height=300, margin=dict(l=20, r=20, t=50, b=20))

    # ASINë³„ NSS vs CAS ì‚°ì ë„
    # ASINë³„ NSS vs CAS ì‚°ì ë„ (Global Comparative Analysis)
    # ë©”ëª¨ë¦¬ ë¬¸ì œë¡œ ì „ì²´ ë°ì´í„°(df_consumer) ë¡œë”©ì„ ì•ˆí•˜ë¯€ë¡œ, 
    # ë¹„êµ ë¶„ì„ ëŒ€ì‹  í˜„ì¬ ê²€ìƒ‰ëœ ìƒí’ˆë“¤ì˜ ë¶„í¬ë§Œ ë³´ì—¬ì£¼ê±°ë‚˜, DB ì§‘ê³„ê°€ í•„ìš”í•¨.
    # ì—¬ê¸°ì„œëŠ” ê²€ìƒ‰ëœ ë°ì´í„°(filtered) ë‚´ì˜ ASINë“¤ë§Œ ë¹„êµí•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶•ì†Œ.
    try:
        asin_stats = filtered.groupby('asin').agg(
            total=('sentiment_score', 'count'),
            pos_count=('sentiment_score', lambda x: (x >= 0.75).sum()),
            neg_count=('sentiment_score', lambda x: (x <= 0.25).sum())
        ).reset_index()
        
        cas_counts = filtered[
            (filtered['repurchase_intent_hybrid'] == True) & 
            (filtered['recommendation_intent_hybrid'] == True)
        ].groupby('asin').size().reset_index(name='adv_count')
        
        asin_stats = pd.merge(asin_stats, cas_counts, on='asin', how='left').fillna(0)
        asin_stats['nss'] = (asin_stats['pos_count'] - asin_stats['neg_count']) / asin_stats['total'] * 100
        asin_stats['cas'] = asin_stats['adv_count'] / asin_stats['total']
    except Exception as e:
        print(f"ASIN Stats Error: {e}")
        asin_stats = pd.DataFrame(columns=['asin', 'nss', 'cas', 'total']) # Empty fallback
    
    current_asins = filtered['asin'].unique()
    fig_scatter_nss = go.Figure()
    fig_scatter_nss.add_trace(go.Scatter(
        x=asin_stats['nss'], y=asin_stats['cas'],
        mode='markers',
        marker=dict(color='lightgray', size=8, opacity=0.5),
        name='íƒ€ì‚¬ ì œí’ˆ'
    ))
    curr_stats = asin_stats[asin_stats['asin'].isin(current_asins)]
    fig_scatter_nss.add_trace(go.Scatter(
        x=curr_stats['nss'], y=curr_stats['cas'],
        mode='markers',
        marker=dict(color='red', size=12, symbol='star'),
        name='í˜„ì¬ ë¶„ì„ ì œí’ˆ'
    ))
    fig_scatter_nss.update_layout(
        title="ë¸Œëœë“œ í¬ì§€ì…”ë‹ (NSS vs CAS)",
        xaxis_title="NSS (ìˆœ ì •ì„œ ì ìˆ˜)",
        yaxis_title="CAS (ê³ ê° ì˜¹í˜¸ ì ìˆ˜)",
        template="plotly_white",
        height=400
    )

    # PQI (Product Quality Index)
    quality_exploded = filtered.explode('quality_issues_semantic')
    quality_issues_count = quality_exploded['quality_issues_semantic'].dropna().value_counts()
    total_issues = quality_issues_count.sum()
    pqi_score = max(0, 100 - (total_issues / total_count * 20)) if total_count > 0 else 100
    
    fig_treemap = go.Figure()
    if not quality_issues_count.empty:
        fig_treemap = px.treemap(
            names=quality_issues_count.index,
            parents=["Quality Issues"] * len(quality_issues_count),
            values=quality_issues_count.values,
            title="ì£¼ìš” í’ˆì§ˆ ë¶ˆë§Œ (Quality Issues)"
        )

    # LFI (Logistics Friction Index)
    lfi_keywords = ['dent', 'leak', 'broken', 'damage', 'crush', 'open']
    lfi_count = 0
    for col in ['delivery_issues_semantic', 'packaging_keywords']:
        if col in filtered.columns:
            exploded = filtered.explode(col)
            mask = exploded[col].astype(str).str.contains('|'.join(lfi_keywords), case=False, na=False)
            lfi_count += mask.sum()
    lfi_rate = (lfi_count / total_count * 100) if total_count > 0 else 0
    
    # SPI (Sensory Performance Index)
    spi_score = (filtered[filtered['sensory_conflict'] == False].shape[0] / total_count * 100) if total_count > 0 else 0
    
    texture_exploded = filtered.explode('texture_terms')
    texture_sentiment = texture_exploded.groupby('texture_terms')['sentiment_score'].mean().sort_values(ascending=False).head(8)
    
    fig_radar = go.Figure()
    if not texture_sentiment.empty:
        categories = texture_sentiment.index.tolist()
        values = texture_sentiment.values.tolist()
        fig_radar = go.Figure(data=go.Scatterpolar(
            r=values + [values[0]],
            theta=categories + [categories[0]],
            fill='toself',
            name='Texture Sentiment'
        ))
        fig_radar.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
            title="ì‹ê°ë³„ ì„ í˜¸ë„ (Textural Preference)",
            height=400
        )

    # Value & Price
    value_score = filtered['value_perception_hybrid'].mean()
    price_sensitive_ratio = filtered['price_sensitive'].mean() if 'price_sensitive' in filtered.columns else 0
    
    marketing_stats = filtered.groupby('asin').agg(
        avg_value=('value_perception_hybrid', 'mean'),
        price_sens=('price_sensitive', 'mean')
    ).reset_index()
    if 'title' in filtered.columns:
        titles = filtered.groupby('asin')['title'].first().reset_index()
        marketing_stats = pd.merge(marketing_stats, titles, on='asin', how='left')
    else:
        marketing_stats['title'] = marketing_stats['asin']
    
    fig_marketing = go.Figure()
    fig_marketing.add_trace(go.Scatter(
        x=marketing_stats['price_sens'], y=marketing_stats['avg_value'],
        mode='markers', text=marketing_stats['title'],
        marker=dict(color='#8884d8', opacity=0.5), name='íƒ€ì‚¬ ì œí’ˆ'
    ))
    curr_mk = marketing_stats[marketing_stats['asin'].isin(current_asins)]
    fig_marketing.add_trace(go.Scatter(
        x=curr_mk['price_sens'], y=curr_mk['avg_value'],
        mode='markers', text=curr_mk['title'],
        marker=dict(color='#ff7300', size=15, symbol='diamond'), name='í˜„ì¬ ì œí’ˆ'
    ))
    fig_marketing.add_hline(y=0, line_dash="dash", line_color="gray")
    fig_marketing.add_vline(x=0.5, line_dash="dash", line_color="gray")
    fig_marketing.update_layout(title="ê°€ì¹˜-ê°€ê²© í¬ì§€ì…”ë‹ ë§µ", xaxis_title="ê°€ê²© ë¯¼ê°ë„", yaxis_title="ê°€ì¹˜ ì¸ì‹", template="plotly_white")

    # â˜… DB ì»¬ëŸ¼ ê¸°ë°˜ ì‹ê°/í˜ì–´ë§ ë¶„ì„
    feature_data = analyze_features(filtered)

    # â˜… í…ìŠ¤íŠ¸ íŒ¨í„´ ë§¤ì¹­ ë³´ì™„ (DB ì»¬ëŸ¼ ë°ì´í„°ê°€ ë¶€ì¡±í•  ë•Œ)
    if 'cleaned_text' in filtered.columns:
        review_texts = filtered['cleaned_text'].dropna().tolist()
        text_pairing = extract_specific_insights(review_texts, mode='pairing')
        text_texture = extract_specific_insights(review_texts, mode='texture')
    else:
        text_pairing = []
        text_texture = []

    result = {
        "has_data": True,
        "search_term": item_name if item_name else item_id,
        "insights": insights_data,
        "metrics": {
            # Frontend expected metrics
            "total_reviews": total_count,
            "impact_score": metrics.get("impact_score", 0),
            "sentiment_z_score": metrics.get("sentiment_z_score", 0),
            "satisfaction_index": metrics.get("satisfaction_index", 0),
            # Additional metrics
            "nss": round(nss_score, 2),
            "cas": round(cas_score, 2),
            "pqi": round(pqi_score, 2),
            "lfi": round(lfi_rate, 2),
            "spi": round(spi_score, 2),
            "value_score": round(value_score, 2),
            "price_sensitivity": round(price_sensitive_ratio, 2)
        },
        "keywords_analysis": keywords_analysis[:50],
        "feature_analysis": {
            "top_textures": feature_data.get("top_textures", []),
            "top_pairings": feature_data.get("top_pairings", []),
            "text_pairing_insights": text_pairing,
            "text_texture_insights": text_texture
        },
        "diverging_summary": {
            "negative_keywords": [{"keyword": k["keyword"], "impact_score": k["impact_score"], "satisfaction_index": k.get("satisfaction_index", 0), "positivity_rate": k.get("positivity_rate", 0), "sample_reviews": k.get("sample_reviews", [])} for k in neg_keywords],
            "positive_keywords": [{"keyword": k["keyword"], "impact_score": k["impact_score"], "satisfaction_index": k.get("satisfaction_index", 0), "positivity_rate": k.get("positivity_rate", 0), "sample_reviews": k.get("sample_reviews", [])} for k in pos_keywords]
        },
        "charts": {
            "impact_diverging_bar": json.loads(fig_diverging.to_json()),
            "sentiment_gap": json.loads(fig_sentiment_gap.to_json()),
            "keyword_rating_corr": json.loads(fig_keyword_rating.to_json()),
            "positivity_bar": json.loads(fig_positivity.to_json()),
            "value_radar": json.loads(fig_radar.to_json()),
            "nss_gauge": json.loads(fig_nss.to_json()),
            "nss_cas_scatter": json.loads(fig_scatter_nss.to_json()),
            "quality_treemap": json.loads(fig_treemap.to_json()),
            "marketing_matrix": json.loads(fig_marketing.to_json())
        }
    }

    # [Added] Business Insights Charts Integration
    try:
        business_insights = generate_business_insights(filtered)
        if "charts" in result:
             result["charts"].update(business_insights)
    except Exception as e:
        print(f"Business Insights Generation Failed: {e}")

    return result

@app.get("/dashboard")
async def dashboard():
    if df is None or df.empty:
        return {"has_data": False}
        
    try:
        # 1. Top 5 êµ­ê°€ ìˆ˜ì¶œ ì¶”ì„¸ (Line)
        # êµ­ê°€ë³„, ì›”ë³„ í•©ì‚°
        country_trend = df.groupby(['period_str', 'country_name'])['export_value'].sum().reset_index()
        # ì´ ìˆ˜ì¶œì•¡ ê¸°ì¤€ Top 5 êµ­ê°€ ì„ ì •
        top_countries = df.groupby('country_name')['export_value'].sum().nlargest(5).index.tolist()
        country_trend_top = country_trend[country_trend['country_name'].isin(top_countries)]
        
        fig1 = px.line(country_trend_top, x='period_str', y='export_value', color='country_name',
                       title="1. Top 5 êµ­ê°€ ìˆ˜ì¶œ ì¶”ì„¸ (Market Trend)")
        fig1.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20), xaxis_title="ê¸°ê°„", yaxis_title="ìˆ˜ì¶œì•¡ ($)")

        # 2. Top 5 í’ˆëª© ìˆ˜ì¶œ ì¶”ì„¸ (Line)
        item_trend = df.groupby(['period_str', 'item_name'])['export_value'].sum().reset_index()
        top_items = df.groupby('item_name')['export_value'].sum().nlargest(5).index.tolist()
        item_trend_top = item_trend[item_trend['item_name'].isin(top_items)]
        
        # UI ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
        item_trend_top['ui_name'] = item_trend_top['item_name'].apply(lambda x: CSV_TO_UI_ITEM_MAPPING.get(x, x))
        
        fig2 = px.line(item_trend_top, x='period_str', y='export_value', color='ui_name',
                       title="2. Top 5 í’ˆëª© ìˆ˜ì¶œ ì¶”ì„¸ (Product Lifecycle)")
        fig2.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20), xaxis_title="ê¸°ê°„", yaxis_title="ìˆ˜ì¶œì•¡ ($)")

        # 3. êµ­ê°€ë³„ í‰ê·  ë‹¨ê°€ ë¹„êµ (Bar - Profitability)
        # ë‹¨ê°€ = ì´ ìˆ˜ì¶œì•¡ / ì´ ì¤‘ëŸ‰ (ì¤‘ëŸ‰ ì—†ìœ¼ë©´ unit_price í‰ê·  ëŒ€ìš©)
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ unit_priceì˜ í‰ê· ì„ êµ­ê°€ë³„ë¡œ ë¹„êµ
        profitability = df.groupby('country_name')['unit_price'].mean().sort_values(ascending=False).reset_index()
        
        fig3 = px.bar(profitability, x='country_name', y='unit_price', color='unit_price',
                      title="3. êµ­ê°€ë³„ í‰ê·  ë‹¨ê°€ (Profitability Check)", color_continuous_scale='Viridis')
        fig3.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20), xaxis_title="êµ­ê°€", yaxis_title="í‰ê·  ë‹¨ê°€ ($/kg)")

        # 4. ì‹œì¥ í¬ì§€ì…”ë‹ ë§µ (Scatter - Volume vs Value)
        # êµ­ê°€ë³„ ì´ ìˆ˜ì¶œì•¡(Value) vs ì´ ì¤‘ëŸ‰(Volume)
        positioning = df.groupby('country_name').agg({
            'export_value': 'sum',
            'export_weight': 'sum'
        }).reset_index()
        
        fig4 = px.scatter(positioning, x='export_weight', y='export_value', text='country_name',
                          size='export_value', color='country_name',
                          title="4. ì‹œì¥ í¬ì§€ì…”ë‹ (Volume vs Value)")
        fig4.update_traces(textposition='top center')
        fig4.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20), 
                           xaxis_title="ì´ ë¬¼ëŸ‰ (Volume)", yaxis_title="ì´ ê¸ˆì•¡ (Value)")

        # 5. í’ˆëª©ë³„ ì›”ë³„ ê³„ì ˆì„± (Heatmap)
        # ì›”(Month) ì¶”ì¶œ
        df['month'] = df['period_str'].apply(lambda x: x.split('-')[1] if '-' in str(x) else '00')
        seasonality = df[df['item_name'].isin(top_items)].groupby(['item_name', 'month'])['export_value'].sum().reset_index()
        
        # UI ì´ë¦„ ë§¤í•‘
        seasonality['ui_name'] = seasonality['item_name'].apply(lambda x: CSV_TO_UI_ITEM_MAPPING.get(x, x))
        
        # Pivot for Heatmap: Index=Item, Columns=Month, Values=ExportValue
        heatmap_data = seasonality.pivot(index='ui_name', columns='month', values='export_value').fillna(0)
        # ì›” ìˆœì„œ ì •ë ¬
        sorted_months = sorted(heatmap_data.columns)
        heatmap_data = heatmap_data[sorted_months]
        
        fig5 = px.imshow(heatmap_data, labels=dict(x="ì›” (Month)", y="í’ˆëª©", color="ìˆ˜ì¶œì•¡"),
                         title="5. ê³„ì ˆì„± ë¶„ì„ (Seasonality Heatmap)", aspect="auto", color_continuous_scale='OrRd')
        fig5.update_layout(template="plotly_white", margin=dict(l=20, r=20, t=40, b=20))

        return {
            "has_data": True,
            "charts": {
                "top_countries": json.loads(fig1.to_json()),
                "top_items": json.loads(fig2.to_json()),
                "profitability": json.loads(fig3.to_json()),
                "positioning": json.loads(fig4.to_json())
            }
        }
    except Exception as e:
        print(f"Dashboard Error: {e}")
        return {"has_data": False, "error": str(e)}

if __name__ == "__main__":
    import uvicorn
    for route in app.routes:
        print(f"Route: {route.path} {route.name}")
    uvicorn.run(app, host="0.0.0.0", port=8000)
