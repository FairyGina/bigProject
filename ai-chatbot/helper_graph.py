# helper_graph.py
from __future__ import annotations

import os
import re
from pathlib import Path
from typing import Any, Dict, List, Tuple, TypedDict

from langgraph.graph import StateGraph, END
from dotenv import load_dotenv

load_dotenv()  # Load .env for local development

try:
    from openai import OpenAI
except Exception:
    OpenAI = None


KB_PATH = Path(__file__).resolve().parent / "kb" / "flow.md"


class HelperState(TypedDict, total=False):
    history: List[Tuple[str | None, str | None]]  # Gradio Chatbot: list of (user, bot)
    intro_done: bool
    user_input: str | None
    kb_text: str
    kb_sections: List[Dict[str, Any]]


def _tokenize(text: str) -> List[str]:
    # í•œê¸€/ì˜ë¬¸/ìˆ«ìž í† í°ë§Œ ë½‘ê¸°
    return re.findall(r"[A-Za-z0-9ê°€-íž£]+", (text or "").lower())


def _load_kb_text() -> str:
    if KB_PATH.exists():
        return KB_PATH.read_text(encoding="utf-8")
    return ""


def _split_kb_sections(md: str) -> List[Dict[str, Any]]:
    """
    ## í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë‚˜ëˆ”
    ì„¹ì…˜: {title, body, tokens}
    """
    md = md or ""
    lines = md.splitlines()

    sections: List[Dict[str, Any]] = []
    cur_title = "KB"
    cur_body: List[str] = []

    def flush():
        nonlocal cur_title, cur_body
        body = "\n".join(cur_body).strip()
        if body:
            tokens = set(_tokenize(cur_title + "\n" + body))
            sections.append({"title": cur_title.strip(), "body": body, "tokens": tokens})
        cur_body = []

    for ln in lines:
        if ln.strip().startswith("## "):
            flush()
            cur_title = ln.strip()[3:].strip()
        else:
            cur_body.append(ln)
    flush()
    return sections


def retrieve_kb(md_sections: List[Dict[str, Any]], question: str, top_k: int = 3) -> List[Dict[str, Any]]:
    q_tokens = set(_tokenize(question))
    if not q_tokens:
        return []

    scored = []
    for sec in md_sections:
        overlap = len(q_tokens & sec.get("tokens", set()))
        if overlap <= 0:
            continue
        # ì œëª© ë§¤ì¹­ ê°€ì¤‘ì¹˜
        title_tokens = set(_tokenize(sec.get("title", "")))
        overlap_title = len(q_tokens & title_tokens)
        score = overlap + overlap_title * 2
        scored.append((score, sec))

    scored.sort(key=lambda x: x[0], reverse=True)
    return [s for _, s in scored[:top_k]]


def _build_context(hits: List[Dict[str, Any]]) -> str:
    blocks = []
    for h in hits:
        blocks.append(f"[{h['title']}]\n{h['body']}".strip())
    return "\n\n---\n\n".join(blocks).strip()


def build_kb_only_answer(question: str, hits: List[Dict[str, Any]]) -> str:
    if not hits:
        return "KBì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì–´ëŠ í™”ë©´ì—ì„œ ë³´ê³  ê³„ì‹ ì§€ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìžˆë‚˜ìš”?"

    sec = hits[0]
    title = (sec.get("title") or "").strip()
    body = (sec.get("body") or "").strip()
    lines = [ln.strip() for ln in body.splitlines() if ln.strip()]

    picked: List[str] = []
    for prefix in ("ë©”ë‰´ ê²½ë¡œ:", "ì§„ìž… ê²½ë¡œ:", "ìƒë‹¨ ì œëª©:", "í™”ë©´ ì„¤ëª…:"):
        for ln in lines:
            if ln.startswith(prefix):
                picked.append(ln)
                break

    if not picked and lines:
        picked = lines[:3]

    header = title or "ê´€ë ¨ í™”ë©´"
    answer = header
    if picked:
        answer += "\n" + "\n".join(picked)
    if title:
        answer += f"\n(KB: {title})"
    return answer.strip()


import sys
import traceback

# ë¡œê±° ì„¤ì • ëŒ€ì‹  sys.stderr.write ì‚¬ìš© (Docker ë¡œê·¸ ê°•ì œ ì¶œë ¥) + íŒŒì¼ ë¡œê·¸ ì¶”ê°€
def log_stderr(msg: str):
    # 1. stderr ì¶œë ¥
    sys.stderr.write(f"[HelperBot] {msg}\n")
    sys.stderr.flush()
    
    # 2. íŒŒì¼ ì¶œë ¥ (í™•ì‹¤í•œ ë””ë²„ê¹…ìš©)
    try:
        with open("/app/debug.log", "a", encoding="utf-8") as f:
            f.write(f"[HelperBot] {msg}\n")
    except Exception:
        pass  # íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨ëŠ” ë¬´ì‹œ

def answer_with_llm(question: str, context: str) -> str:
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    
    # ë””ë²„ê¹…: API Key ë° Model í™•ì¸ (í•„ìˆ˜)
    if api_key:
        log_stderr(f"API Key found (starts with: {api_key[:8]}..., len={len(api_key)})")
    else:
        log_stderr("CRITICAL: OPENAI_API_KEY is EMPTY!")
        
    log_stderr(f"Using Model: {model}")

    if not api_key or OpenAI is None:
        if context:
            return (
                "í˜„ìž¬ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ KB ê¸°ë°˜ìœ¼ë¡œë§Œ ì•ˆë‚´í•©ë‹ˆë‹¤.\n\n"
                + context
            ).strip()
        return "OPENAI_API_KEYê°€ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í™˜ê²½ë³€ìˆ˜ í™•ì¸ í•„ìš”)"

    client = OpenAI(api_key=api_key)

    system = (
        "ë„ˆëŠ” 'í™ˆíŽ˜ì´ì§€ FAQ / ì‚¬ìš©ë°©ë²• ë„ìš°ë¯¸' ì±—ë´‡ì´ë‹¤.\n"
        "- ì‚¬ìš©ìžëŠ” ì›¹ì‚¬ì´íŠ¸ë¥¼ ì´ìš© ì¤‘ì´ë©°, ë©”ë‰´ ê²½ë¡œ(ì¢Œì¸¡ ë©”ë‰´)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì•ˆë‚´í•œë‹¤.\n"
        "- ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, ì§§ê³  ì •í™•í•˜ê²Œ, ë‹¨ê³„(1,2,3)ë¡œ ì•ˆë‚´í•œë‹¤.\n"
        "- ì œê³µëœ KB ë‚´ìš© ì•ˆì—ì„œë§Œ ê·¼ê±°ë¥¼ ë‘ê³ , ì¶”ì¸¡í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
    )

    user = f"""
[ì§ˆë¬¸]
{question}

[KB ì»¨í…ìŠ¤íŠ¸]
{context if context else "(ê´€ë ¨ KB ì—†ìŒ)"}

ìš”êµ¬ì‚¬í•­:
- 'ì–´ë””ì„œ/ì–´ë–»ê²Œ'ë¥¼ ë©”ë‰´ ê²½ë¡œë¡œ ëª…í™•ížˆ ì•ˆë‚´
- ê·¼ê±°ê°€ ë˜ëŠ” ì„¹ì…˜ ì œëª©ì„ (KB: ì„¹ì…˜ëª…) ìœ¼ë¡œ í‘œê¸°
""".strip()

    try:
        log_stderr(f"Sending request to OpenAI... (Question: {question})")
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
        )
        log_stderr("OpenAI API call successful")
        return resp.choices[0].message.content.strip()
    except Exception as e:
        error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n[ì—ëŸ¬ ë‚´ìš©]: {str(e)}\n"
        import traceback
        tb = traceback.format_exc()
        log_stderr(f"OpenAI API Call Failed: {e}\n{tb}")
        
        # ë””ë²„ê¹…ìš©: UIì— ì—ëŸ¬ ë…¸ì¶œ
        return f"{error_msg}\n\n[Traceback]\n{tb}"


def intro_node(state: Dict[str, Any]) -> Dict[str, Any]:
    if state.get("intro_done"):
        return state

    kb_text = _load_kb_text()
    sections = _split_kb_sections(kb_text)

    state["kb_text"] = kb_text
    state["kb_sections"] = sections
    state["history"] = state.get("history") or []
    state["history"].append((None, "ì•ˆë…•í•˜ì„¸ìš”! ðŸ‘‹\ní™ˆíŽ˜ì´ì§€ FAQ / ì‚¬ìš©ë°©ë²• ë„ìš°ë¯¸ìž…ë‹ˆë‹¤.\nê¶ê¸ˆí•œ ê¸°ëŠ¥ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."))
    if not kb_text.strip():
        state["history"].append((None, "â€» í˜„ìž¬ KB(flow.md)ê°€ ë¹„ì–´ ìžˆì–´ìš”. kb/flow.mdì— í™”ë©´/ë©”ë‰´ ì•ˆë‚´ë¥¼ ì¶”ê°€í•´ ì£¼ì„¸ìš”."))
    state["intro_done"] = True
    return state


def answer_node(state: Dict[str, Any]) -> Dict[str, Any]:
    q = (state.get("user_input") or "").strip()
    log_stderr(f"Received help request: {q}")
    if not q:
        return state

    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    use_llm = bool(api_key) and OpenAI is not None

    sections = state.get("kb_sections") or []
    hits = retrieve_kb(sections, q, top_k=3 if use_llm else 1)
    context = _build_context(hits)

    if not context:
        # KB ë§¤ì¹­ ì‹¤íŒ¨ â†’ ì•ˆë‚´ ìš”ì²­
        msg = (
            "KBì— ë”± ë§žëŠ” ì•ˆë‚´ë¥¼ ëª» ì°¾ì•˜ì–´ìš”.\n"
            "ì§€ê¸ˆ ë³´ê³  ìžˆëŠ” í™”ë©´ ì´ë¦„(ì˜ˆ: ê³µì§€ì‚¬í•­/ë ˆì‹œí”¼ í—ˆë¸Œ/ìµœì¢… ë ˆì‹œí”¼ ì„ ì •/ë³´ê³ ì„œ í™”ë©´)ì´ë‚˜\n"
            "ì¢Œì¸¡ ë©”ë‰´ ê²½ë¡œë¥¼ ê°™ì´ ì•Œë ¤ì£¼ë©´ ì •í™•ížˆ ì•ˆë‚´í• ê²Œìš”."
        )
        state["history"].append((q, msg))
        state["user_input"] = None
        return state

    if not use_llm:
        ans = build_kb_only_answer(q, hits)
    else:
        ans = answer_with_llm(q, context)

    # ê·¼ê±° ì„¹ì…˜ëª… ìµœì†Œ í‘œê¸°(LLMì´ ì¨ì£¼ì§€ë§Œ í˜¹ì‹œ ì—†ìœ¼ë©´ ê°•ì œë¡œ)
    if "(KB:" not in ans:
        titles = ", ".join([h["title"] for h in hits[:2]])
        ans = ans.rstrip() + f"\n(KB: {titles})"

    state["history"].append((q, ans))
    state["user_input"] = None
    return state


graph = StateGraph(dict)
graph.add_node("intro", intro_node)
graph.add_node("answer", answer_node)

graph.set_entry_point("intro")
graph.add_edge("intro", "answer")
graph.add_edge("answer", END)

compiled = graph.compile()


def make_initial_state() -> Dict[str, Any]:
    return {
        "history": [],
        "intro_done": False,
        "user_input": None,
        "kb_text": "",
        "kb_sections": [],
    }
